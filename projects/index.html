<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Yuhang Zhao </title> <meta name="author" content="Yuhang Zhao"> <meta name="description" content="Assistant Professor of Computer Science at University of Wisconsin-Madison. I create intelligent interactive systems to enhance human abilities. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?64a8851ed36aad74b39738c8658b6a71"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhangzhao1.github.io/projects/"> <script src="/assets/js/theme.js?e7b9db9c4115dd1adca7dbd0349227ff"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuhang</span> Zhao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/">Recruitment </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">MadAbility Lab </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-sun-filled" id="light-toggle-dark"></i> <i class="ti ti-moon-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/LrKdzvQqDkc?si=QDRYQ7BjuDZxiSqY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="Chen2025VisiMark" class="col-sm-8"> <div class="title">VisiMark: Characterizing and Augmenting Landmarks for People with Low Vision in Augmented Reality to Support Indoor Navigation</div> <div class="title"> <em>CHI 2025</em>   </div> <div class="author"> Ruijia Chen, Junru Jiang, Pragati Maheshwary, Brianna R. Cochran, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2502.10561" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Landmarks are critical in navigation, supporting self-orientation and mental model development. Similar to sighted people, people with low vision (PLV) frequently look for landmarks via visual cues but face difficulties identifying some important landmarks due to vision loss. We first conducted a formative study with six PLV to characterize their challenges and strategies in landmark selection, identifying their unique landmark categories (e.g., area silhouettes, accessibility-related objects) and preferred landmark augmentations. We then designed <i>VisiMark</i>, an AR interface that supports landmark perception for PLV by providing both overviews of space structures and in-situ landmark augmentations. We evaluated VisiMark with 16 PLV and found that VisiMark enabled PLV to perceive landmarks they preferred but could not easily perceive before, and changed PLV’s landmark selection from only visually-salient objects to cognitive landmarks that are more important and meaningful. We further derive design considerations for AR-based landmark augmentation systems for PLV.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatarlib-480.webp 480w,/assets/img/publication_preview/avatarlib-800.webp 800w,/assets/img/publication_preview/avatarlib-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/avatarlib.png" class="preview" width="150" height="130" alt="avatarlib.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zhang2025Inclusive" class="col-sm-8"> <div class="title">Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality</div> <div class="title"> <em>CHI 2025</em>   </div> <div class="author"> Kexin Zhang, Edward Glenn Scott Spencer, Andric Li, Ang Li, Yaxing Yao, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2502.09811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/MadisonAbilityLab/Inclusive-Avatar-Guidelines-and-Library" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Open Source</a> </div> <div class="abstract hidden"> <p>Avatar is a critical medium for identity representation in social virtual reality (VR). However, options for disability expression are highly limited on current avatar interfaces. Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update. Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/goalexploration-480.webp 480w,/assets/img/publication_preview/goalexploration-800.webp 800w,/assets/img/publication_preview/goalexploration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/goalexploration.png" class="preview" width="150" height="130" alt="goalexploration.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Povinelli2025Beyond" class="col-sm-8"> <div class="title">Beyond the "Industry Standard": Focusing Gender-Affirming Voice Training Technologies on Individualized Goal Exploration</div> <div class="title"> <em>CHI 2025</em>   </div> <div class="author"> Kassie Povinelli, Hanxiu "Hazel" Zhu, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2410.09958" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Gender-affirming voice training is critical for the transition process for many transgender individuals, enabling their voice to align with their gender identity. Individualized voice goals guide and motivate the voice training journey, but existing voice training technologies fail to define clear goals. We interviewed six voice experts and ten transgender individuals with voice training experience (voice trainees), focusing on how they defined, triangulated, and used voice goals. We found that goal voice exploration involves navigation between approximate and clear goals, and continuous reevaluation throughout the voice training journey. Our study reveals how voice examples, character descriptions, and voice modification and training technologies inform goal exploration, and identifies risks of overemphasizing goals. We identified technological implications informed by the separation of voice goals and targets, and provide guidelines for for supporting individualized goals throughout the voice training journey based on brainstorming with trainees and experts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/5ctxSHTA-do?si=SpjlJ5HKNtwt5jjy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="lee2024CookAR" class="col-sm-8"> <div class="title">CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision</div> <div class="title"> <em>UIST 2024</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Belonging and Inclusion Best Paper Award</em> </div> <div class="author"> Jaewook Lee, Andrew D Tjahjadi, Jiho Kim, Junpu Yu, Minji Park, Jiawen Zhang, Jon E. Froehlich, Yapeng Tian, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3654777.3676449" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/makeabilitylab/CookAR?tab=readme-ov-file" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Open Source</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Belonging and Inclusion Best Paper Award</p> </div> <div class="abstract hidden"> <p>Cooking is a central activity of daily living, supporting independence as well as mental and physical health. However, prior work has highlighted key barriers for people with low vision (LV) to cook, particularly around safely interacting with tools, such as sharp knives or hot pans. Drawing on recent advancements in computer vision (CV), we present <i>CookAR</i>, a head-mounted AR system with real-time object affordance augmentations to support safe and efficient interactions with kitchen tools. To design and implement CookAR, we collected and annotated the first egocentric dataset of kitchen tool affordances, fine-tuned an affordance segmentation model, and developed an AR system with a stereo camera to generate visual augmentations. To validate CookAR, we conducted a technical evaluation of our fine-tuned model as well as a qualitative lab study with 10 LV participants for suitable augmentation design. Our technical evaluation demonstrates that our model outperforms the baseline on our tool affordance dataset, while our user study indicates a preference for affordance augmentations over the traditional whole object augmentations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AIVision-480.webp 480w,/assets/img/publication_preview/AIVision-800.webp 800w,/assets/img/publication_preview/AIVision-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/AIVision.png" class="preview" width="150" height="130" alt="AIVision.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2024AIVision" class="col-sm-8"> <div class="title">AI-Vision: A Three-Layer Accessible Image Exploration System For People with Visual Impairments in China</div> <div class="title"> <em>IMWUT 2024</em>   </div> <div class="author"> Kaixing Zhao, Rui Lai, Bin Guo, Le Liu, Liang He, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3678537" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Accessible image exploration systems are able to help people with visual impairments (PVI) to understand image content by providing different types of interactions. With the development of computer vision technologies, image exploration systems are supporting more fine-grained image content processing, including image segmentation, description and object recognition. However, in developing countries like China, it is still rare for PVI to widely rely on such accessible system. To better understand the usage situation of accessible image exploration system in China and improve the image understanding of PVI in China, we developed AI-Vision, an Android based hierarchical accessible image exploration system supporting the generations of image general description, local object description and metadata information. Our 7-day diary study with 10 PVI verified the usability of AI-Vision and also revealed a series of design implications for improving accessible image exploration systems similar to AI-Vision.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hri-480.webp 480w,/assets/img/publication_preview/hri-800.webp 800w,/assets/img/publication_preview/hri-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/hri.png" class="preview" width="150" height="130" alt="hri.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hu2024really" class="col-sm-8"> <div class="title">"This Really Lets Us See the Entire World:" Designing a Conversational Telepresence Robot for Homebound Older Adults</div> <div class="title"> <em>DIS 2024</em>   </div> <div class="author"> Yaxin Hu, Laura Stegner, Yasmine Kotturi, Caroline Zhang, Yi-Hao Peng, Faria Huq, <em>Yuhang Zhao</em>, Jeffrey P Bigham, and Bilge Mutlu </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3643834.3660710" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we explore the design and use of conversational telepresence robots to help homebound older adults interact with the external world. An initial needfinding study (N=8) using video vignettes revealed older adults’ experiential needs for robot-mediated remote experiences such as exploration, reminiscence and social participation. We then designed a prototype system to support these goals and conducted a technology probe study (N=11) to garner a deeper understanding of user preferences for remote experiences. The study revealed user interactive patterns in each desired experience, highlighting the need of robot guidance, social engagements with the robot and the remote bystanders. Our work identifies a novel design space where conversational telepresence robots can be used to foster meaningful interactions in the remote physical environment. We offer design insights into the robot’s proactive role in providing guidance and using dialogue to create personalized, contextualized and meaningful experiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/SMbDfQtHQRE?si=-jXfU1VZ3Pifz38y" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="wang2024gazeprompt" class="col-sm-8"> <div class="title">GazePrompt: Enhancing Low Vision People’s Reading Experience with Gaze-Aware Augmentations</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Ru Wang, Zach Potter, Yun Ho, Daniel Killough, Linda Zeng, Sanbrita Mondal, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3613904.3642878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Reading is a challenging task for low vision people. While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. To fill this gap, we present <b>GazePrompt</b>, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users’ gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with. Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants’ line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users’ concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NIST-480.webp 480w,/assets/img/publication_preview/NIST-800.webp 800w,/assets/img/publication_preview/NIST-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/NIST.png" class="preview" width="150" height="130" alt="NIST.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2024exploring" class="col-sm-8"> <div class="title">Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Kexin Zhang, Brianna R Cochran, Ruijia Chen, Lance Hargung, Bryce Sprecher, Ross Tredinnick, Kevin Ponto, Suman Banerjee, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>First responders navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. %First responders perform dangerous and time sensitive tasks in \changemass-casualty incidents hazardous, unfamiliar environments, making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design spaces have not been thoroughly explored by involving various first responders. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed different first responders’ unique preferences and needs for AR cues and interactions, and identified desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spica-480.webp 480w,/assets/img/publication_preview/spica-800.webp 800w,/assets/img/publication_preview/spica-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/spica.png" class="preview" width="150" height="130" alt="spica.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ning2024Spica" class="col-sm-8"> <div class="title">SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Zheng Ning, Brianna L Wimer, Kaiwen Jiang, Jerrick Ban, Yapeng Tian, <em>Yuhang Zhao</em>, and Toby Jia-Jun Li </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642632" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce SPICA, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, SPICA offers novel interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, SPICA augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of SPICA and explored user behaviors, preferences, and mental models when interacting with augmented ADs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lowvisionvis-480.webp 480w,/assets/img/publication_preview/lowvisionvis-800.webp 800w,/assets/img/publication_preview/lowvisionvis-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/lowvisionvis.png" class="preview" width="150" height="130" alt="lowvisionvis.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wang2024lowvision" class="col-sm-8"> <div class="title">How Do Low-Vision Individuals Experience Information Visualization?</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Yanan Wang, <em>Yuhang Zhao</em>, and Yea-Seul Kim </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642188" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, there has been a growing interest in enhancing the accessibility of visualizations for people with visual impairments. While much of the research has focused on improving accessibility for screen reader users, the specific needs of people with remaining vision (i.e., low-vision individuals) have been largely unaddressed. To bridge this gap, we conducted a qualitative study that provides insights into how low-vision individuals experience visualizations. We found that participants utilized various strategies to examine visualizations using the screen magnifiers and also observed that the default zoom level participants use for general purposes may not be optimal for reading visualizations. We identified that participants relied on their prior knowledge and memory to minimize the traversing cost when examining visualization. Based on the findings, we motivate a personalized tool to accommodate varying visual conditions of low-vision individuals and derive the design goals and features of the tool.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/hbHJVuG1Hfs?si=gZaJvq1OlYh1Nsb7" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="Bei2024Starrescue" class="col-sm-8"> <div class="title">StarRescue: the Design and Evaluation of A Turn-Taking Collaborative Game for Facilitating Autistic Children’s Social Skills</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Rongqi Bei, Yajie Liu, Yihe Wang, Yuxuan Huang, Ming Li, <em>Yuhang Zhao</em>, and Xin Tong </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Autism Spectrum Disorder (ASD) presents challenges in social interaction skill development, particularly in turn-taking. Digital interventions offer potential solutions for improving autistic children’s social skills but often lack addressing specific collaboration techniques. Therefore, we designed a prototype of a turn-taking collaborative tablet game, StarRescue, which encourages children’s distinct collaborative roles and interdependence while progressively enhancing sharing and mutual planning skills. We further conducted a controlled study with 32 autistic children to evaluate StarRescue’s usability and potential effectiveness in improving their social skills. Findings indicated that StarRescue has great potential to foster turn-taking skills and social communication skills and also extended beyond the game. Additionally, we discussed implications for future work, such as including parents as game spectators and understanding autistic children’s territory awareness in collaboration. Our study contributes a promising digital intervention for autistic children’s turn-taking social skill development via a scaffolding approach and valuable design implications for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/voicechanger-480.webp 480w,/assets/img/publication_preview/voicechanger-800.webp 800w,/assets/img/publication_preview/voicechanger-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/voicechanger.png" class="preview" width="150" height="130" alt="voicechanger.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="povinelli2024springboard" class="col-sm-8"> <div class="title">Springboard, Roadblock or "Crutch"?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality</div> <div class="title"> <em>IEEE VR 2024</em>   </div> <div class="author"> Kassie Povinelli, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494190" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/1bdxNmVLZy8?si=2fsNY32gODTY5Syy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="he2023multi" class="col-sm-8"> <div class="title">A Multi-modal Toolkit to Support DIY Assistive Technology Creation for Blind and Low Vision People</div> <div class="title"> <em>UIST 2023 Demo</em>   </div> <div class="author"> Liwen He, Yifan Li, Mingming Fan, Liang He, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3586182.3616646?casa_token=3e0TLtHQdpsAAAAA:-N0QUWy5m9qYYsq5fuwJEv58iwIU8ivPNOrw6A21anuVr4hMMcztkp2ahqHMbEcAt4r4qbG4y88iAA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We design and build A11yBits, a tangible toolkit that empowers blind and low vision (BLV) people to easily create personalized do-it-yourself assistive technologies (DIY-ATs). A11yBits includes (1) a series of Sensing modules to detect both environmental information and user commands, (2) a set of Feedback modules to send multi-modal feedback, and (3) two Base modules (Sensing Base and Feedback Base) to power and connect the sensing and feedback modules. The toolkit enables accessible and easy assembly via a “plug-and-play” mechanism. BLV users can select and assemble their preferred modules to create personalized DIY-ATs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatar-480.webp 480w,/assets/img/publication_preview/avatar-800.webp 800w,/assets/img/publication_preview/avatar-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/avatar.png" class="preview" width="150" height="130" alt="avatar.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023diary" class="col-sm-8"> <div class="title">A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities</div> <div class="title"> <em>ASSETS 2023</em>   </div> <div class="author"> Kexin Zhang, Elmira Deldari, Yaxing Yao, and <em>Yuhang Zhao</em> </div> <div class="links"> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388?casa_token=jLUHVfut7IgAAAAA:nenvMgIl7_F5YiBLHVdg_wicjNnATnoz7MdWa5_RDGI_ClXjFBCXuUTC0xh6zmDdUNYz2L8Sq4UuLA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instructions-480.webp 480w,/assets/img/publication_preview/instructions-800.webp 800w,/assets/img/publication_preview/instructions-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/instructions.png" class="preview" width="150" height="130" alt="instructions.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023practices" class="col-sm-8"> <div class="title">Practices and Barriers of Cooking Training for Blind and Low Vision People</div> <div class="title"> <em>ASSETS 2023</em>   </div> <div class="author"> Ru Wang, Nihan Zhou, Tam Nguyen, Sanbrita Mondal, Bilge Mutlu, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.05396" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3614494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gazeoverlay-480.webp 480w,/assets/img/publication_preview/gazeoverlay-800.webp 800w,/assets/img/publication_preview/gazeoverlay-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/gazeoverlay.jpeg" class="preview" width="150" height="130" alt="gazeoverlay.jpeg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023understanding" class="col-sm-8"> <div class="title">Understanding How Low Vision People Read Using Eye Tracking</div> <div class="title"> <em>CHI 2023</em>   </div> <div class="author"> Ru Wang, Linxiu Zeng, Xinyong Zhang, Sanbrita Mondal, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581213?casa_token=65SwRttGZ3YAAAAA:Y8ZG_9uzrSQTQNfwjg_oKgokmo_stRcHJVRN7_i56WFLWSumf29oeRGBO8i2_lg9voe7BLkOxb4KpEc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While being able to read with screen magnifiers, low vision people have slow and unpleasant reading experiences.Eye tracking has the potential to improve their experience by recognizing fine-grained gaze behaviors and providing more targeted enhancements. To inspire gaze-based low vision technology, <b>we investigate the suitable method to collect low vision users’ gaze data via commercial eye trackers and thoroughly explore their challenges in reading based on their gaze behaviors</b>. With an improved calibration interface, we collected the gaze data of 20 low vision participants and 20 sighted controls who performed reading tasks on a computer screen; low vision participants were also asked to read with different screen magnifiers. We found that, with an accessible calibration interface and data collection method, commercial eye trackers can collect gaze data of comparable quality from low vision and sighted people. Our study identified low vision people’s unique gaze patterns during reading, building upon which, we propose design implications for gaze-based low vision technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bystander-480.webp 480w,/assets/img/publication_preview/bystander-800.webp 800w,/assets/img/publication_preview/bystander-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/bystander.png" class="preview" width="150" height="130" alt="bystander.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2023if" class="col-sm-8"> <div class="title">"If Sighted People Know, I Should Be Able to Know:" Privacy Perceptions of Bystanders with Visual Impairments around Camera-based Technology</div> <div class="title"> <em>USENIX Security 2023</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Yaxing Yao, Jiaru Fu, and Nihan Zhou </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhao-yuhang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Camera-based technology can be privacy-invasive, especially for bystanders who can be captured by the cameras but do not have direct control or access to the devices. The privacy threats become even more significant to bystanders with visual impairments (BVI) since they cannot visually discover the use of cameras nearby and effectively avoid being captured. While some prior research has studied visually impaired people’s privacy concerns as direct users of camera-based assistive technologies, no research has explored their unique privacy perceptions and needs as bystanders. We conducted an in-depth interview study with 16 visually impaired participants to understand BVI’s privacy concerns, expectations, and needs in different camera usage scenarios. A preliminary survey with 90 visually impaired respondents and 96 sighted controls was conducted to compare BVI and sighted bystanders’ general attitudes towards cameras and elicit camera usage scenarios for the interview study. Our research revealed BVI’s unique privacy challenges and perceptions around cameras, highlighting their needs for privacy awareness and protection. We summarized design considerations for future privacy-enhancing technologies to fulfill BVI’s privacy needs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/1ITzOttCJkc?si=RiN1ntNEcplD9U5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="ji2022vrbubble" class="col-sm-8"> <div class="title">VRBubble: Enhancing Peripheral Awareness of Avatars for People with Visual Impairments in Social Virtual Reality</div> <div class="title"> <em>ASSETS 2022, CHI 2022 LBW</em>   </div> <div class="author"> Tiger F Ji, Brianna Cochran, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3517428.3544821" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://dl.acm.org/doi/10.1145/3491101.3519657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Social Virtual Reality (VR) is growing for remote socializing and collaboration. However, current social VR applications are not accessible to people with visual impairments (PVI) due to their focus on visual experiences. We aim to facilitate social VR accessibility by enhancing PVI’s peripheral awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based VR technique that provides surrounding avatar information based on social distances. Based on Hall’s proxemic theory, VRBubble divides the social space with three Bubbles—Intimate, Conversation, and Social Bubble—generating spatial audio feedback to distinguish avatars in different bubbles and provide suitable avatar information. We provide three audio alternatives: earcons, verbal notifications, and real-world sound effects. PVI can select and combine their preferred feedback alternatives for different avatars, bubbles, and social contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatarrepresent-480.webp 480w,/assets/img/publication_preview/avatarrepresent-800.webp 800w,/assets/img/publication_preview/avatarrepresent-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/avatarrepresent.png" class="preview" width="150" height="130" alt="avatarrepresent.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022s" class="col-sm-8"> <div class="title">“It’s Just Part of Me:” Understanding Avatar Diversity and Self-presentation of People with Disabilities in Social Virtual Reality</div> <div class="title"> <em>ASSETS 2022</em>   </div> <div class="author"> Kexin Zhang, Elmira Deldari, Zhicong Lu, Yaxing Yao, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3517428.3544829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD’s avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/robot-480.webp 480w,/assets/img/publication_preview/robot-800.webp 800w,/assets/img/publication_preview/robot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/robot.png" class="preview" width="150" height="130" alt="robot.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bhat2022confused" class="col-sm-8"> <div class="title">"I was Confused by It; It was Confused by Me:" Exploring the Experiences of People with Visual Impairments around Mobile Service Robots</div> <div class="title"> <em>CSCW 2022</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Diversity and Inclusion Recognition</em> </div> <div class="author"> Prajna Bhat, and <em>Yuhang Zhao</em> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3555582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Diversity and Inclusion Recognition</p> </div> <div class="abstract hidden"> <p>Mobile service robots have become increasingly ubiquitous. However, these robots can pose potential accessibility issues and safety concerns to people with visual impairments (PVI). We sought to explore the challenges faced by PVI around mainstream mobile service robots and identify their needs. Seventeen PVI were interviewed about their experiences with three emerging robots: vacuum robots, delivery robots, and drones. We comprehensively investigated PVI’s robot experiences by considering their different roles around robots—direct users and bystanders. Our study highlighted participants’ challenges and concerns about the accessibility, safety, and privacy issues around mobile service robots. We found that the lack of accessible feedback made it difficult for PVI to precisely control, locate, and track the status of the robots. Moreover, encountering mobile robots as bystanders confused and even scared the participants, presenting safety and privacy barriers. We further distilled design considerations for more accessible and safe robots for PVI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sok-480.webp 480w,/assets/img/publication_preview/sok-800.webp 800w,/assets/img/publication_preview/sok-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/sok.png" class="preview" width="150" height="130" alt="sok.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="stephenson2022sok" class="col-sm-8"> <div class="title">Sok: Authentication in Augmented and Virtual Reality</div> <div class="title"> <em>IEEE S&amp;P 2022</em>   </div> <div class="author"> Sophie Stephenson, Bijeeta Pal, Stephen Fan, Earlence Fernandes, <em>Yuhang Zhao</em>, and Rahul Chatterjee </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9833742?casa_token=lUPsA-7ItPoAAAAA:iHe0NSjfYDaRLXZZtYKlnHj5sDnc_2-I2OA20v9DussDup2Ia2tWiyk7lCiph-cdCCneZC%E2%80%93IQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Augmented reality (AR) and virtual reality (VR) devices are emerging as prominent contenders to today’s personal computers. As personal devices, users will use AR and VR to store and access their sensitive data and thus will need secure and usable ways to authenticate. In this paper, we evaluate the state-of-the-art of authentication mechanisms for AR/VR devices by systematizing research efforts and practical deployments. By studying users’ experiences with authentication on AR and VR, we gain insight into the important properties needed for authentication on these devices. We then use these properties to perform a comprehensive evaluation of AR/VR authentication mechanisms both proposed in literature and used in practice. In all, we synthesize a coherent picture of the current state of authentication mechanisms for AR/VR devices. We draw on our findings to provide concrete research directions and advice on implementing and evaluating future authentication methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vis-480.webp 480w,/assets/img/publication_preview/vis-800.webp 800w,/assets/img/publication_preview/vis-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/vis.png" class="preview" width="150" height="130" alt="vis.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jung2021communicating" class="col-sm-8"> <div class="title">Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments</div> <div class="title"> <em>IEEE VIS 2021</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> Crescentia Jung, Shubham Mehta, Atharva Kulkarni, <em>Yuhang Zhao</em>, and Yea-Seul Kim </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9552938?casa_token=DEkn5d1il8cAAAAA:8HL2neCYlFi_yKCBT4yqC5Vz-r1k2Gao6l1OHECi5A4JETvSAEhfO_v7TxpekmD2LfSe3vOWwQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Honorable Mention</p> </div> <div class="abstract hidden"> <p>Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users’ cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/facesight-480.webp 480w,/assets/img/publication_preview/facesight-800.webp 800w,/assets/img/publication_preview/facesight-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/facesight.png" class="preview" width="150" height="130" alt="facesight.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weng2021facesight" class="col-sm-8"> <div class="title">Facesight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-facing Camera Vision</div> <div class="title"> <em>CHI 2021</em>   </div> <div class="author"> Yueting Weng, Chun Yu, Yingtian Shi, <em>Yuhang Zhao</em>, Yukang Yan, and Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445484?casa_token=RaRZ-I6Jb74AAAAA:6nD-lPNAxiq94gSjHNsndk0magOHkQ7SIgQ3HEUUfMXQGeF-ZFTCgHtNQnB3y4hwNIcYySs-y_STzg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/edu-480.webp 480w,/assets/img/publication_preview/edu-800.webp 800w,/assets/img/publication_preview/edu-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/edu.png" class="preview" width="150" height="130" alt="edu.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2020teacher" class="col-sm-8"> <div class="title">Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities</div> <div class="title"> <em>ASSETS 2020</em>   </div> <div class="author"> Zikai Alex Wen, Erica Silverstein, <em>Yuhang Zhao</em>, Anjelika Lynne Amog, Katherine Garnett, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3373625.3417029?casa_token=dDs3L4VS9tMAAAAA:Z7LLIcb8lSXmjGcZ__6QA6SJW3B_9L_GO68zkV5W5MKG9NO6HBcNGIjwaBm4ddwEZ7TvCaSWwW6I-Q" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many students with specific learning disabilities (SLDs) have difficulty learning math. To succeed in math, they need to receive personalized support from teachers. Recently, math e-learning tools that provide personalized math skills training have gained popularity. However, we know little about how well these tools help teachers personalize instruction for students with SLDs. To answer this question, we conducted semi-structured interviews with 12 teachers who taught students with SLDs in grades five to eight. We found that participants used math e-learning tools that were not designed specifically for students with SLDs. Participants had difficulty using these tools because of text-intensive user interfaces, insufficient feedback about student performance, inability to adjust difficulty levels, and problems with setup and maintenance. Participants also needed assistive technology for their students, but they had challenges in getting and using it. From our findings, we distilled design implications to help shape the design of more inclusive and effective e-learning tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/wayfinding-480.webp 480w,/assets/img/publication_preview/wayfinding-800.webp 800w,/assets/img/publication_preview/wayfinding-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/wayfinding.png" class="preview" width="150" height="130" alt="wayfinding.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2020effectiveness" class="col-sm-8"> <div class="title">The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision</div> <div class="title"> <em>CHI 2020</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Elizabeth Kupferstein, Hathaitorn Rojnirun, Leah Findlater, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376516?casa_token=tjQeRWaX2ZsAAAAA:FjjX9Zm0XK-ypL57ajwALW3HKfgPuK3qypcU9W3s54Ekpx-6f8ww-INKa84MGpv5pNzTriJ_IHq6GA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Wayfinding is a critical but challenging task for people who have low vision, a visual impairment that falls short of blindness. Prior wayfinding systems for people with visual impairments focused on blind people, providing only audio and tactile feedback. Since people with low vision use their remaining vision, we sought to determine how audio feedback compares to visual feedback in a wayfinding task. We developed visual and audio wayfinding guidance on smartglasses based on de facto standard approaches for blind and sighted people and conducted a study with 16 low vision participants. We found that participants made fewer mistakes and experienced lower cognitive load with visual feedback. Moreover, participants with a full field of view completed the wayfinding tasks faster when using visual feedback. However, many participants preferred audio feedback because of its shorter learning curve. We propose design guidelines for wayfinding systems for low vision.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/molder-480.webp 480w,/assets/img/publication_preview/molder-800.webp 800w,/assets/img/publication_preview/molder-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/molder.png" class="preview" width="150" height="130" alt="molder.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2020molder" class="col-sm-8"> <div class="title">Molder: an Accessible Design Tool for Tactile Maps</div> <div class="title"> <em>CHI 2020</em>   </div> <div class="author"> Lei Shi, <em>Yuhang Zhao</em>, Ricardo Gonzalez Penuela, Elizabeth Kupferstein, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376431" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&amp;M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&amp;M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/izmKY17CDhg?si=1mqWSMUC_qnAMU0x" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2019seeingvr" class="col-sm-8"> <div class="title">SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision</div> <div class="title"> <em>CHI 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Edward Cutrell, Christian Holz, Meredith Ringel Morris, Eyal Ofek, and Andrew D Wilson </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300341?casa_token=F95sK5_FZU0AAAAA:G04PVw98T40fHibgDJfk6bOv67nPxXsuQEc6-vxWNa3GjpHuX_-_oMyj7c7tfVfyeo0YCg8ClIyKDQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/microsoft/SeeingVRtoolkit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Open Source</a> </div> <div class="abstract hidden"> <p>Current virtual reality applications do not support people who have low vision, i.e., vision loss that falls short of complete blindness but is not correctable by glasses. We present SeeingVR, a set of 14 tools that enhance a VR application for people with low vision by providing visual and audio augmentations. A user can select, adjust, and combine different tools based on their preferences. Nine of our tools modify an existing VR application post hoc via a plugin without developer effort. The rest require simple inputs from developers using a Unity toolkit we created that allows integrating all 14 of our low vision support tools during development. Our evaluation with 11 participants with low vision showed that SeeingVR enabled users to better enjoy VR and complete tasks more quickly and accurately. Developers also found our Unity toolkit easy and convenient to use.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/BbZNsa1ZeFQ?si=ghO2UiTbtblW00vN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2019designing" class="col-sm-8"> <div class="title">Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision</div> <div class="title"> <em>UIST 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Elizabeth Kupferstein, Brenda Veronica Castro, Steven Feiner, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3332165.3347906" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don’t typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user’s position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants’ walking speed. Our designs on both platforms largely increased participants’ self-reported psychological security.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/foreseeplus-480.webp 480w,/assets/img/publication_preview/foreseeplus-800.webp 800w,/assets/img/publication_preview/foreseeplus-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/foreseeplus.png" class="preview" width="150" height="130" alt="foreseeplus.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2019designinh" class="col-sm-8"> <div class="title">Designing and Evaluating a Customizable Head-mounted Vision Enhancement System for People with Low Vision</div> <div class="title"> <em>TACCESS 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Sarit Szpiro, Lei Shi, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3361866" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in head-mounted displays (HMDs) present an opportunity to design vision enhancement systems for people with low vision, whose vision cannot be corrected with glasses or contact lenses. We aim to understand whether and how HMDs can aid low vision people in their daily lives. We designed ForeSee, an HMD prototype that enhances people’s view of the world with image processing techniques such as magnification and edge enhancement. We evaluated these vision enhancements with 20 low vision participants who performed four viewing tasks: image recognition and reading tasks from near- and far-distance. We found that participants needed to combine and adjust the enhancements to comfortably complete the viewing tasks. We then designed two input modes to enable fast and easy customization: speech commands and smartwatch-based gestures. While speech commands are commonly used for eyes-free input, our novel set of onscreen gestures on a smartwatch can be used in scenarios where speech is not appropriate or desired. We evaluated both input modes with 11 low vision participants and found that both modes effectively enabled low vision users to customize their visual experience on the HMD. We distill design insights for HMD applications for low vision and spur new research directions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beautiful-480.webp 480w,/assets/img/publication_preview/beautiful-800.webp 800w,/assets/img/publication_preview/beautiful-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/beautiful.png" class="preview" width="150" height="130" alt="beautiful.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2018looks" class="col-sm-8"> <div class="title">"It Looks Beautiful but Scary": How Low Vision People Navigate Stairs and Other Surface Level Changes</div> <div class="title"> <em>ASSETS 2018</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> <em>Yuhang Zhao</em>, Elizabeth Kupferstein, Doron Tal, and Shiri Azenkot </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3234695.3236359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Honorable Mention</p> </div> <div class="abstract hidden"> <p>Walking in environments with stairs and curbs is potentially dangerous for people with low vision. We sought to understand what challenges low vision people face and what strategies and tools they use when navigating such surface level changes. Using contextual inquiry, we interviewed and observed 14 low vision participants as they completed navigation tasks in two buildings and through two city blocks. The tasks involved walking in- and outdoors, across four staircases and two city blocks. We found that surface level changes were a source of uncertainty and even fear for all participants. Besides the white cane that many participants did not want to use, participants did not use technology in the study. Participants mostly used their vision, which was exhausting and sometimes deceptive. Our findings highlight the need for systems that support surface level changes and other depth-perception tasks; they should consider low vision people’s distinct experiences from blind people, their sensitivity to different lighting conditions, and leverage visual enhancements.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/sjitW-XYxrs?si=yVJZoCENbkyQVTIJ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2018enabling" class="col-sm-8"> <div class="title">Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation</div> <div class="title"> <em>CHI 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Cynthia L Bennett, Hrvoje Benko, Edward Cutrell, Christian Holz, Meredith Ringel Morris, and Mike Sinclair </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3173574.3173690" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/face-480.webp 480w,/assets/img/publication_preview/face-800.webp 800w,/assets/img/publication_preview/face-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/face.png" class="preview" width="150" height="130" alt="face.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2018face" class="col-sm-8"> <div class="title">A Face Recognition Application for People with Visual Impairments: Understanding Use beyond the Lab</div> <div class="title"> <em>CHI 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Shaomei Wu, Lindsay Reynolds, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3173574.3173789?casa_token=1ttDg814xkoAAAAA:dxQULHeu4tgDGZXAhDhgJ5Ed3elXwvbv4VWv7dEWaSFP_lmgYnemPylgF1c0aymZUvAXhcPiXWo_DQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user’s friends’ tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone’s camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone’s status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/description-480.webp 480w,/assets/img/publication_preview/description-800.webp 800w,/assets/img/publication_preview/description-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/description.png" class="preview" width="150" height="130" alt="description.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2017effect" class="col-sm-8"> <div class="title">The Effect of Computer-generated Descriptions on Photo-sharing Experiences of People with Visual Impairments</div> <div class="title"> <em>CSCW 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Shaomei Wu, Lindsay Reynolds, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3134756?casa_token=irNy8hKlgDEAAAAA:GWkq8Q_7QDE6VNFqZKSZpueYqaE6FCCeix25LzsHbkH3QQu8euSbkyY46yQ4r_-brO4Sedlg7uDmnA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Like sighted people, visually impaired people want to share photographs on social networking services, but find it difficult to identify and select photos from their albums. We aimed to address this problem by incorporating state-of-the-art computer-generated descriptions into Facebook’s photo-sharing feature. We interviewed 12 visually impaired participants to understand their photo-sharing experiences and designed a photo description feature for the Facebook mobile application. We evaluated this feature with six participants in a seven-day diary study. We found that participants used the descriptions to recall and organize their photos, but they hesitated to upload photos without a sighted person’s input. In addition to basic information about photo content, participants wanted to know more details about salient objects and people, and whether the photos reflected their personal aesthetic. We discuss these findings from the lens of self-disclosure and self-presentation theories and propose new computer vision research directions that will better support visual content sharing by visually impaired people.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/models-480.webp 480w,/assets/img/publication_preview/models-800.webp 800w,/assets/img/publication_preview/models-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/models.png" class="preview" width="150" height="130" alt="models.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2017designing" class="col-sm-8"> <div class="title">Designing Interactions for 3D Printed Models with Blind People</div> <div class="title"> <em>ASSETS 2017</em>   </div> <div class="author"> Lei Shi, <em>Yuhang Zhao</em>, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3132525.3132549?casa_token=TFuTN_SmmJcAAAAA:fNzYa7-4nCuqsp2DC_HUHKUHJTThYQAhAGNwr1ylDiS0MgnFAPD10kmMTIH1T0Kj4iEUa4BroFfarQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users’ needs, and explore design opportunities. In the first section of the study, we observed participants’ behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants’ exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/markit" sizes="95vw"></source> <img src="/assets/img/publication_preview/markit" class="preview" width="150" height="130" alt="markit" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2017markit" class="col-sm-8"> <div class="title">Markit and Talkit: a Low-barrier Toolkit to Augment 3D Printed Models with Audio Annotations</div> <div class="title"> <em>UIST 2017</em>   </div> <div class="author"> Lei Shi, <em>Yuhang Zhao</em>, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3126594.3126650?casa_token=BpTRFwCMXusAAAAA:qYYcOnB9R3Fr7wGv636oxEm0pkEE-11dY5_4PFbU8Ys4_W92UTHQqED7jXLBzMHteU1NOUK49OHKKQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit’s verbal cues. Talkit uses an RGB camera and a microphone to sense users’ inputs so it can run on a variety of devices. We evaluated Markit with eight sighted "makers" and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ARpotential-480.webp 480w,/assets/img/publication_preview/ARpotential-800.webp 800w,/assets/img/publication_preview/ARpotential-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/ARpotential.png" class="preview" width="150" height="130" alt="ARpotential.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2017understanding" class="col-sm-8"> <div class="title">Understanding Low Vision People’s Visual Perception on Commercial Augmented Reality Glasses</div> <div class="title"> <em>CHI 2017</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Michele Hu, Shafeka Hashash, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3025453.3025949?casa_token=aBgHPVW-B1IAAAAA:TutyBJh8RPtYAXffiLl7iHZpicj7o2metZtJ_OMs1xyLiT8DRaCy3P1vh9tWOmkekwmW9xpQxRmZjA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device’s constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people’s walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/device-480.webp 480w,/assets/img/publication_preview/device-800.webp 800w,/assets/img/publication_preview/device-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/device.png" class="preview" width="150" height="130" alt="device.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="szpiro2016people" class="col-sm-8"> <div class="title">How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities</div> <div class="title"> <em>ASSETS 2016</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> Sarit Felicia Anais Szpiro, Shafeka Hashash, <em>Yuhang Zhao</em>, and Shiri Azenkot </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/2982142.2982168?casa_token=yPY7rJWqn0AAAAAA:TPrZjAabnaNaj2sH3Uiv2dsme7sJmex8Y-QWO0sxmZCjbMmlqk0JMY77EZl3jVL4kjNz_dvNUmFHCw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Honorable Mention</p> </div> <div class="abstract hidden"> <p>Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/store-480.webp 480w,/assets/img/publication_preview/store-800.webp 800w,/assets/img/publication_preview/store-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/store.png" class="preview" width="150" height="130" alt="store.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="szpiro2016finding" class="col-sm-8"> <div class="title">Finding a Store, Searching for a Product: a Study of Daily Challenges of Low Vision People</div> <div class="title"> <em>UbiComp 2016</em>   </div> <div class="author"> Sarit Szpiro, <em>Yuhang Zhao</em>, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/2971648.2971723?casa_token=gqavoyDXWSMAAAAA:Vuk27wqkmnJcSfZUU5wgEukQBUpQXqieUEdCRkE0jUR698JImjF4ohniaujNkjlKc2SbhV1ngAo-Qg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual impairments encompass a range of visual abilities. People with low vision have functional vision and thus their experiences are likely to be different from people with no vision. We sought to answer two research questions: (1) what challenges do low vision people face when performing daily activities and (2) what aids (high- and low-tech) do low vision people use to alleviate these challenges? Our goal was to reveal gaps in current technologies that can be addressed by the UbiComp community. Using contextual inquiry, we observed 11 low vision people perform a wayfinding and shopping task in an unfamiliar environment. The task involved wayfinding and searching and purchasing a product. We found that, although there are low vision aids on the market, participants mostly used their smartphones, despite interface accessibility challenges. While smartphones helped them outdoors, participants were overwhelmed and frustrated when shopping in a store. We discuss the inadequacies of existing aids and highlight the need for systems that enhance visual information, rather than convert it to audio or tactile.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/0b8Q4832bjk?si=hzxGsfTwlAki_P4G" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2016cuesee" class="col-sm-8"> <div class="title">CueSee: Exploring Visual Cues for People with Low Vision to Facilitate a Visual Search Task</div> <div class="title"> <em>UbiComp 2016</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Sarit Szpiro, Jonathan Knighten, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2971648.2971730" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual search is a major challenge for low vision people. Conventional vision enhancements like magnification help low vision people see more details, but cannot indicate the location of a target in a visual search task. In this paper, we explore visual cues—a new approach to facilitate visual search tasks for low vision people. We focus on product search and present CueSee, an augmented reality application on a head-mounted display (HMD) that facilitates product search by recognizing the product automatically and using visual cues to direct the user’s attention to the product. We designed five visual cues that users can combine to suit their visual condition. We evaluated the visual cues with 12 low vision participants and found that participants preferred using our cues to conventional enhancements for product search. We also found that CueSee outperformed participants’ best-corrected vision in both time and accuracy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/foresee-480.webp 480w,/assets/img/publication_preview/foresee-800.webp 800w,/assets/img/publication_preview/foresee-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/foresee.png" class="preview" width="150" height="130" alt="foresee.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2015foresee" class="col-sm-8"> <div class="title">Foresee: A Customizable Head-mounted Vision Enhancement System for People with Low Vision</div> <div class="title"> <em>ASSETS 2015</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Sarit Szpiro, and Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2700648.2809865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most low vision people have functional vision and would likely prefer to use their vision to access information. Recently, there have been advances in head-mounted displays, cameras, and image processing technology that create opportunities to improve the visual experience for low vision people. In this paper, we present ForeSee, a head-mounted vision enhancement system with five enhancement methods: Magnification, Contrast Enhancement, Edge Enhancement, Black/White Reversal, and Text Extraction; in two display modes: Full and Window. ForeSee enables users to customize their visual experience by selecting, adjusting, and combining different enhancement methods and display modes in real time. We evaluated ForeSee by conducting a study with 19 low vision participants who performed near- and far-distance viewing tasks. We found that participants had different preferences for enhancement methods and display modes when performing different tasks. The Magnification Enhancement Method and the Window Display Mode were popular choices, but most participants felt that combining several methods produced the best results. The ability to customize the system was key to enabling people with a variety of different vision abilities to improve their visual experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/-h1reziSo-Y?si=QNFjeFqKC16KSYZD" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2014qook" class="col-sm-8"> <div class="title">QOOK: Enhancing Information Revisitation for Active Reading with a Paper Book</div> <div class="title"> <em>TEI 2014</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Yongqiang Qin, Yang Liu, Siqi Liu, Taoshuai Zhang, and Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2540930.2540977" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Revisiting information on previously accessed pages is a common activity during active reading. Both physical and digital books have their own benefits in supporting such activity according to their manipulation natures. In this paper, we introduce QOOK, a paper-book based interactive reading system, which integrates the advanced technology of digital books with the affordances of physical books to facilitate people’s information revisiting process. The design goals of QOOK are derived from the literature survey and our field study on physical and digital books respectively. QOOK allows page flipping just like on a real book and enables people to use electronic functions such as keyword searching, highlighting and bookmarking. A user study is conducted and the study results demonstrate that QOOK brings faster information revisiting and better reading experience to readers.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/ydlAKS-WlqU?si=2IK3Ow2lM9uzvRZh" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="huang2014focus" class="col-sm-8"> <div class="title">FOCUS: Enhancing Children’s Engagement in Reading by Using Contextual BCI Training Sessions</div> <div class="title"> <em>CHI 2014</em>   </div> <div class="author"> Jin Huang, Chun Yu, Yuntao Wang, <em>Yuhang Zhao</em>, Siqi Liu, Chou Mo, Jie Liu, Lie Zhang, and Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2556288.2557339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Reading is an important aspect of a child’s development. Reading outcome is heavily dependent on the level of engagement while reading. In this paper, we present FOCUS, an EEG-augmented reading system which monitors a child’s engagement level in real time, and provides contextual BCI training sessions to improve a child’s reading engagement. A laboratory experiment was conducted to assess the validity of the system. Results showed that FOCUS could significantly improve engagement in terms of both EEG-based measurement and teachers’ subjective measure on the reading outcome.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/HupyGn2rJLo?si=x_QH0DWfaZn09rYI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2011picopet" class="col-sm-8"> <div class="title">PicoPet: " Real World" Digital Pet on a Handheld Projector</div> <div class="title"> <em>UIST 2011 Adjunct</em>   </div> <div class="author"> <em>Yuhang Zhao</em>, Chao Xue, Xiang Cao, and Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2046396.2046398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player’s daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuhang Zhao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>