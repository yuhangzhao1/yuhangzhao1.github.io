<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | Yuhang Zhao </title> <meta name="author" content="Yuhang Zhao"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?64a8851ed36aad74b39738c8658b6a71"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhangzhao1.github.io/projects/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuhang </span> Zhao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">MadAbility Lab </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <article> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/GazePrompt-16-9-480.webp 480w,/assets/img/publication_preview/GazePrompt-16-9-800.webp 800w,/assets/img/publication_preview/GazePrompt-16-9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/GazePrompt-16-9.gif" class="preview" width="150" height="130" alt="GazePrompt-16-9.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024gazeprompt" class="col-sm-8"> <div class="title">GazePrompt: Enhancing Low Vision People’s Reading Experience with Gaze-Aware Augmentations</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Ru Wang ,  Zach Potter ,  Yun Ho ,  Daniel Killough ,  Linda Zeng ,  Sanbrita Mondal ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Reading is a challenging task for low vision people. While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. To fill this gap, we present <b>GazePrompt</b>, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users’ gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with. Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants’ line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users’ concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NIST-480.webp 480w,/assets/img/publication_preview/NIST-800.webp 800w,/assets/img/publication_preview/NIST-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/NIST.png" class="preview" width="150" height="130" alt="NIST.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2024exploring" class="col-sm-8"> <div class="title">Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Kexin Zhang ,  Brianna R Cochran ,  Ruijia Chen ,  Lance Hargung ,  Bryce Sprecher ,  Ross Tredinnick ,  Kevin Ponto ,  Suman Banerjee ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>First responders navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. %First responders perform dangerous and time sensitive tasks in \changemass-casualty incidents hazardous, unfamiliar environments, making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design spaces have not been thoroughly explored by involving various first responders. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed different first responders’ unique preferences and needs for AR cues and interactions, and identified desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spica-480.webp 480w,/assets/img/publication_preview/spica-800.webp 800w,/assets/img/publication_preview/spica-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/spica.png" class="preview" width="150" height="130" alt="spica.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ning2024Spica" class="col-sm-8"> <div class="title">SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Zheng Ning ,  Brianna L Wimer ,  Kaiwen Jiang ,  Jerrick Ban ,  Yapeng Tian ,  <em>Yuhang Zhao</em> ,  and  Toby Jia-Jun Li </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce SPICA, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, SPICA offers novel interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, SPICA augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of SPICA and explored user behaviors, preferences, and mental models when interacting with augmented ADs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lowvisionvis-480.webp 480w,/assets/img/publication_preview/lowvisionvis-800.webp 800w,/assets/img/publication_preview/lowvisionvis-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/lowvisionvis.png" class="preview" width="150" height="130" alt="lowvisionvis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Wang2024lowvision" class="col-sm-8"> <div class="title">How Do Low-Vision Individuals Experience Information Visualization?</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Yanan Wang ,  <em>Yuhang Zhao</em> ,  and  Yea-Seul Kim </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, there has been a growing interest in enhancing the accessibility of visualizations for people with visual impairments. While much of the research has focused on improving accessibility for screen reader users, the specific needs of people with remaining vision (i.e., low-vision individuals) have been largely unaddressed. To bridge this gap, we conducted a qualitative study that provides insights into how low-vision individuals experience visualizations. We found that participants utilized various strategies to examine visualizations using the screen magnifiers and also observed that the default zoom level participants use for general purposes may not be optimal for reading visualizations. We identified that participants relied on their prior knowledge and memory to minimize the traversing cost when examining visualization. Based on the findings, we motivate a personalized tool to accommodate varying visual conditions of low-vision individuals and derive the design goals and features of the tool.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/hbHJVuG1Hfs?si=gZaJvq1OlYh1Nsb7" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="Bei2024Starrescue" class="col-sm-8"> <div class="title">StarRescue: the Design and Evaluation of A Turn-Taking Collaborative Game for Facilitating Autistic Children’s Social Skills</div> <div class="title"> <em>CHI 2024</em>   </div> <div class="author"> Rongqi Bei ,  Yajie Liu ,  Yihe Wang ,  Yuxuan Huang ,  Ming Li ,  <em>Yuhang Zhao</em> ,  and  Xin Tong </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Autism Spectrum Disorder (ASD) presents challenges in social interaction skill development, particularly in turn-taking. Digital interventions offer potential solutions for improving autistic children’s social skills but often lack addressing specific collaboration techniques. Therefore, we designed a prototype of a turn-taking collaborative tablet game, StarRescue, which encourages children’s distinct collaborative roles and interdependence while progressively enhancing sharing and mutual planning skills. We further conducted a controlled study with 32 autistic children to evaluate StarRescue’s usability and potential effectiveness in improving their social skills. Findings indicated that StarRescue has great potential to foster turn-taking skills and social communication skills and also extended beyond the game. Additionally, we discussed implications for future work, such as including parents as game spectators and understanding autistic children’s territory awareness in collaboration. Our study contributes a promising digital intervention for autistic children’s turn-taking social skill development via a scaffolding approach and valuable design implications for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/voicechanger-480.webp 480w,/assets/img/publication_preview/voicechanger-800.webp 800w,/assets/img/publication_preview/voicechanger-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/voicechanger.png" class="preview" width="150" height="130" alt="voicechanger.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="povinelli2024springboard" class="col-sm-8"> <div class="title">Springboard, Roadblock or "Crutch"?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality</div> <div class="title"> <em>IEEE VR 2024</em>   </div> <div class="author"> Kassie Povinelli ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatar-480.webp 480w,/assets/img/publication_preview/avatar-800.webp 800w,/assets/img/publication_preview/avatar-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/avatar.png" class="preview" width="150" height="130" alt="avatar.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023diary" class="col-sm-8"> <div class="title">A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities</div> <div class="title"> <em>ASSETS 2023</em>   </div> <div class="author"> Kexin Zhang ,  Elmira Deldari ,  Yaxing Yao ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388?casa_token=jLUHVfut7IgAAAAA:nenvMgIl7_F5YiBLHVdg_wicjNnATnoz7MdWa5_RDGI_ClXjFBCXuUTC0xh6zmDdUNYz2L8Sq4UuLA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instructions-480.webp 480w,/assets/img/publication_preview/instructions-800.webp 800w,/assets/img/publication_preview/instructions-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/instructions.png" class="preview" width="150" height="130" alt="instructions.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023practices" class="col-sm-8"> <div class="title">Practices and Barriers of Cooking Training for Blind and Low Vision People</div> <div class="title"> <em>ASSETS 2023</em>   </div> <div class="author"> Ru Wang ,  Nihan Zhou ,  Tam Nguyen ,  Sanbrita Mondal ,  Bilge Mutlu ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2310.05396" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3614494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gazeoverlay-480.webp 480w,/assets/img/publication_preview/gazeoverlay-800.webp 800w,/assets/img/publication_preview/gazeoverlay-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/gazeoverlay.jpeg" class="preview" width="150" height="130" alt="gazeoverlay.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023understanding" class="col-sm-8"> <div class="title">Understanding How Low Vision People Read Using Eye Tracking</div> <div class="title"> <em>CHI 2023</em>   </div> <div class="author"> Ru Wang ,  Linxiu Zeng ,  Xinyong Zhang ,  Sanbrita Mondal ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581213?casa_token=65SwRttGZ3YAAAAA:Y8ZG_9uzrSQTQNfwjg_oKgokmo_stRcHJVRN7_i56WFLWSumf29oeRGBO8i2_lg9voe7BLkOxb4KpEc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>While being able to read with screen magnifiers, low vision people have slow and unpleasant reading experiences.Eye tracking has the potential to improve their experience by recognizing fine-grained gaze behaviors and providing more targeted enhancements. To inspire gaze-based low vision technology, <b>we investigate the suitable method to collect low vision users’ gaze data via commercial eye trackers and thoroughly explore their challenges in reading based on their gaze behaviors</b>. With an improved calibration interface, we collected the gaze data of 20 low vision participants and 20 sighted controls who performed reading tasks on a computer screen; low vision participants were also asked to read with different screen magnifiers. We found that, with an accessible calibration interface and data collection method, commercial eye trackers can collect gaze data of comparable quality from low vision and sighted people. Our study identified low vision people’s unique gaze patterns during reading, building upon which, we propose design implications for gaze-based low vision technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bystander-480.webp 480w,/assets/img/publication_preview/bystander-800.webp 800w,/assets/img/publication_preview/bystander-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/bystander.png" class="preview" width="150" height="130" alt="bystander.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2023if" class="col-sm-8"> <div class="title">"If Sighted People Know, I Should Be Able to Know:" Privacy Perceptions of Bystanders with Visual Impairments around Camera-based Technology</div> <div class="title"> <em>USENIX Security 2023</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Yaxing Yao ,  Jiaru Fu ,  and  Nihan Zhou </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhao-yuhang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Camera-based technology can be privacy-invasive, especially for bystanders who can be captured by the cameras but do not have direct control or access to the devices. The privacy threats become even more significant to bystanders with visual impairments (BVI) since they cannot visually discover the use of cameras nearby and effectively avoid being captured. While some prior research has studied visually impaired people’s privacy concerns as direct users of camera-based assistive technologies, no research has explored their unique privacy perceptions and needs as bystanders. We conducted an in-depth interview study with 16 visually impaired participants to understand BVI’s privacy concerns, expectations, and needs in different camera usage scenarios. A preliminary survey with 90 visually impaired respondents and 96 sighted controls was conducted to compare BVI and sighted bystanders’ general attitudes towards cameras and elicit camera usage scenarios for the interview study. Our research revealed BVI’s unique privacy challenges and perceptions around cameras, highlighting their needs for privacy awareness and protection. We summarized design considerations for future privacy-enhancing technologies to fulfill BVI’s privacy needs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/1ITzOttCJkc?si=RiN1ntNEcplD9U5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="ji2022vrbubble" class="col-sm-8"> <div class="title">Vrbubble: Enhancing Peripheral Awareness of Avatars for People with Visual Impairments in Social Virtual Reality</div> <div class="title"> <em>ASSETS 2022, CHI 2022 LBW</em>   </div> <div class="author"> Tiger F Ji ,  Brianna Cochran ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3517428.3544821" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://dl.acm.org/doi/10.1145/3491101.3519657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>Social Virtual Reality (VR) is growing for remote socializing and collaboration. However, current social VR applications are not accessible to people with visual impairments (PVI) due to their focus on visual experiences. We aim to facilitate social VR accessibility by enhancing PVI’s peripheral awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based VR technique that provides surrounding avatar information based on social distances. Based on Hall’s proxemic theory, VRBubble divides the social space with three Bubbles—Intimate, Conversation, and Social Bubble—generating spatial audio feedback to distinguish avatars in different bubbles and provide suitable avatar information. We provide three audio alternatives: earcons, verbal notifications, and real-world sound effects. PVI can select and combine their preferred feedback alternatives for different avatars, bubbles, and social contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatarrepresent-480.webp 480w,/assets/img/publication_preview/avatarrepresent-800.webp 800w,/assets/img/publication_preview/avatarrepresent-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/avatarrepresent.png" class="preview" width="150" height="130" alt="avatarrepresent.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022s" class="col-sm-8"> <div class="title">“It’s Just Part of Me:” Understanding Avatar Diversity and Self-presentation of People with Disabilities in Social Virtual Reality</div> <div class="title"> <em>ASSETS 2022</em>   </div> <div class="author"> Kexin Zhang ,  Elmira Deldari ,  Zhicong Lu ,  Yaxing Yao ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3517428.3544829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD’s avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/robot-480.webp 480w,/assets/img/publication_preview/robot-800.webp 800w,/assets/img/publication_preview/robot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/robot.png" class="preview" width="150" height="130" alt="robot.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bhat2022confused" class="col-sm-8"> <div class="title">"I was Confused by It; It was Confused by Me:" Exploring the Experiences of People with Visual Impairments around Mobile Service Robots</div> <div class="title"> <em>CSCW 2022</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Diversity and Inclusion Recognition</em> </div> <div class="author"> Prajna Bhat ,  and  <em>Yuhang Zhao</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3555582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Mobile service robots have become increasingly ubiquitous. However, these robots can pose potential accessibility issues and safety concerns to people with visual impairments (PVI). We sought to explore the challenges faced by PVI around mainstream mobile service robots and identify their needs. Seventeen PVI were interviewed about their experiences with three emerging robots: vacuum robots, delivery robots, and drones. We comprehensively investigated PVI’s robot experiences by considering their different roles around robots—direct users and bystanders. Our study highlighted participants’ challenges and concerns about the accessibility, safety, and privacy issues around mobile service robots. We found that the lack of accessible feedback made it difficult for PVI to precisely control, locate, and track the status of the robots. Moreover, encountering mobile robots as bystanders confused and even scared the participants, presenting safety and privacy barriers. We further distilled design considerations for more accessible and safe robots for PVI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sok-480.webp 480w,/assets/img/publication_preview/sok-800.webp 800w,/assets/img/publication_preview/sok-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/sok.png" class="preview" width="150" height="130" alt="sok.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="stephenson2022sok" class="col-sm-8"> <div class="title">Sok: Authentication in Augmented and Virtual Reality</div> <div class="title"> <em>IEEE S&amp;P 2022</em>   </div> <div class="author"> Sophie Stephenson ,  Bijeeta Pal ,  Stephen Fan ,  Earlence Fernandes ,  <em>Yuhang Zhao</em> ,  and  Rahul Chatterjee </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9833742?casa_token=lUPsA-7ItPoAAAAA:iHe0NSjfYDaRLXZZtYKlnHj5sDnc_2-I2OA20v9DussDup2Ia2tWiyk7lCiph-cdCCneZC%E2%80%93IQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Augmented reality (AR) and virtual reality (VR) devices are emerging as prominent contenders to today’s personal computers. As personal devices, users will use AR and VR to store and access their sensitive data and thus will need secure and usable ways to authenticate. In this paper, we evaluate the state-of-the-art of authentication mechanisms for AR/VR devices by systematizing research efforts and practical deployments. By studying users’ experiences with authentication on AR and VR, we gain insight into the important properties needed for authentication on these devices. We then use these properties to perform a comprehensive evaluation of AR/VR authentication mechanisms both proposed in literature and used in practice. In all, we synthesize a coherent picture of the current state of authentication mechanisms for AR/VR devices. We draw on our findings to provide concrete research directions and advice on implementing and evaluating future authentication methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vis-480.webp 480w,/assets/img/publication_preview/vis-800.webp 800w,/assets/img/publication_preview/vis-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/vis.png" class="preview" width="150" height="130" alt="vis.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jung2021communicating" class="col-sm-8"> <div class="title">Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments</div> <div class="title"> <em>IEEE VIS 2021</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> Crescentia Jung ,  Shubham Mehta ,  Atharva Kulkarni ,  <em>Yuhang Zhao</em> ,  and  Yea-Seul Kim </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9552938?casa_token=DEkn5d1il8cAAAAA:8HL2neCYlFi_yKCBT4yqC5Vz-r1k2Gao6l1OHECi5A4JETvSAEhfO_v7TxpekmD2LfSe3vOWwQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users’ cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/facesight-480.webp 480w,/assets/img/publication_preview/facesight-800.webp 800w,/assets/img/publication_preview/facesight-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/facesight.png" class="preview" width="150" height="130" alt="facesight.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weng2021facesight" class="col-sm-8"> <div class="title">Facesight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-facing Camera Vision</div> <div class="title"> <em>CHI 2021</em>   </div> <div class="author"> Yueting Weng ,  Chun Yu ,  Yingtian Shi ,  <em>Yuhang Zhao</em> ,  Yukang Yan ,  and  Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445484?casa_token=RaRZ-I6Jb74AAAAA:6nD-lPNAxiq94gSjHNsndk0magOHkQ7SIgQ3HEUUfMXQGeF-ZFTCgHtNQnB3y4hwNIcYySs-y_STzg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/edu-480.webp 480w,/assets/img/publication_preview/edu-800.webp 800w,/assets/img/publication_preview/edu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/edu.png" class="preview" width="150" height="130" alt="edu.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2020teacher" class="col-sm-8"> <div class="title">Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities</div> <div class="title"> <em>ASSETS 2020</em>   </div> <div class="author"> Zikai Alex Wen ,  Erica Silverstein ,  <em>Yuhang Zhao</em> ,  Anjelika Lynne Amog ,  Katherine Garnett ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3373625.3417029?casa_token=dDs3L4VS9tMAAAAA:Z7LLIcb8lSXmjGcZ__6QA6SJW3B_9L_GO68zkV5W5MKG9NO6HBcNGIjwaBm4ddwEZ7TvCaSWwW6I-Q" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many students with specific learning disabilities (SLDs) have difficulty learning math. To succeed in math, they need to receive personalized support from teachers. Recently, math e-learning tools that provide personalized math skills training have gained popularity. However, we know little about how well these tools help teachers personalize instruction for students with SLDs. To answer this question, we conducted semi-structured interviews with 12 teachers who taught students with SLDs in grades five to eight. We found that participants used math e-learning tools that were not designed specifically for students with SLDs. Participants had difficulty using these tools because of text-intensive user interfaces, insufficient feedback about student performance, inability to adjust difficulty levels, and problems with setup and maintenance. Participants also needed assistive technology for their students, but they had challenges in getting and using it. From our findings, we distilled design implications to help shape the design of more inclusive and effective e-learning tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/wayfinding-480.webp 480w,/assets/img/publication_preview/wayfinding-800.webp 800w,/assets/img/publication_preview/wayfinding-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/wayfinding.png" class="preview" width="150" height="130" alt="wayfinding.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2020effectiveness" class="col-sm-8"> <div class="title">The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision</div> <div class="title"> <em>CHI 2020</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Elizabeth Kupferstein ,  Hathaitorn Rojnirun ,  Leah Findlater ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376516?casa_token=tjQeRWaX2ZsAAAAA:FjjX9Zm0XK-ypL57ajwALW3HKfgPuK3qypcU9W3s54Ekpx-6f8ww-INKa84MGpv5pNzTriJ_IHq6GA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Wayfinding is a critical but challenging task for people who have low vision, a visual impairment that falls short of blindness. Prior wayfinding systems for people with visual impairments focused on blind people, providing only audio and tactile feedback. Since people with low vision use their remaining vision, we sought to determine how audio feedback compares to visual feedback in a wayfinding task. We developed visual and audio wayfinding guidance on smartglasses based on de facto standard approaches for blind and sighted people and conducted a study with 16 low vision participants. We found that participants made fewer mistakes and experienced lower cognitive load with visual feedback. Moreover, participants with a full field of view completed the wayfinding tasks faster when using visual feedback. However, many participants preferred audio feedback because of its shorter learning curve. We propose design guidelines for wayfinding systems for low vision.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/molder-480.webp 480w,/assets/img/publication_preview/molder-800.webp 800w,/assets/img/publication_preview/molder-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/molder.png" class="preview" width="150" height="130" alt="molder.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2020molder" class="col-sm-8"> <div class="title">Molder: an Accessible Design Tool for Tactile Maps</div> <div class="title"> <em>CHI 2020</em>   </div> <div class="author"> Lei Shi ,  <em>Yuhang Zhao</em> ,  Ricardo Gonzalez Penuela ,  Elizabeth Kupferstein ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376431" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&amp;M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&amp;M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/izmKY17CDhg?si=1mqWSMUC_qnAMU0x" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2019seeingvr" class="col-sm-8"> <div class="title">SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision</div> <div class="title"> <em>CHI 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Edward Cutrell ,  Christian Holz ,  Meredith Ringel Morris ,  Eyal Ofek ,  and  Andrew D Wilson </div> <div class="links"> <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300341?casa_token=F95sK5_FZU0AAAAA:G04PVw98T40fHibgDJfk6bOv67nPxXsuQEc6-vxWNa3GjpHuX_-_oMyj7c7tfVfyeo0YCg8ClIyKDQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/microsoft/SeeingVRtoolkit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/BbZNsa1ZeFQ?si=ghO2UiTbtblW00vN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2019designing" class="col-sm-8"> <div class="title">Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision</div> <div class="title"> <em>UIST 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Elizabeth Kupferstein ,  Brenda Veronica Castro ,  Steven Feiner ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3332165.3347906" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don’t typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user’s position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants’ walking speed. Our designs on both platforms largely increased participants’ self-reported psychological security.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/foreseeplus-480.webp 480w,/assets/img/publication_preview/foreseeplus-800.webp 800w,/assets/img/publication_preview/foreseeplus-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/foreseeplus.png" class="preview" width="150" height="130" alt="foreseeplus.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2019designinh" class="col-sm-8"> <div class="title">Designing and Evaluating a Customizable Head-mounted Vision Enhancement System for People with Low Vision</div> <div class="title"> <em>TACCESS 2019</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Sarit Szpiro ,  Lei Shi ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3361866" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recent advances in head-mounted displays (HMDs) present an opportunity to design vision enhancement systems for people with low vision, whose vision cannot be corrected with glasses or contact lenses. We aim to understand whether and how HMDs can aid low vision people in their daily lives. We designed ForeSee, an HMD prototype that enhances people’s view of the world with image processing techniques such as magnification and edge enhancement. We evaluated these vision enhancements with 20 low vision participants who performed four viewing tasks: image recognition and reading tasks from near- and far-distance. We found that participants needed to combine and adjust the enhancements to comfortably complete the viewing tasks. We then designed two input modes to enable fast and easy customization: speech commands and smartwatch-based gestures. While speech commands are commonly used for eyes-free input, our novel set of onscreen gestures on a smartwatch can be used in scenarios where speech is not appropriate or desired. We evaluated both input modes with 11 low vision participants and found that both modes effectively enabled low vision users to customize their visual experience on the HMD. We distill design insights for HMD applications for low vision and spur new research directions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beautiful-480.webp 480w,/assets/img/publication_preview/beautiful-800.webp 800w,/assets/img/publication_preview/beautiful-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/beautiful.png" class="preview" width="150" height="130" alt="beautiful.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2018looks" class="col-sm-8"> <div class="title">"It Looks Beautiful but Scary": How Low Vision People Navigate Stairs and Other Surface Level Changes</div> <div class="title"> <em>ASSETS 2018</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> <em>Yuhang Zhao</em> ,  Elizabeth Kupferstein ,  Doron Tal ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3234695.3236359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Walking in environments with stairs and curbs is potentially dangerous for people with low vision. We sought to understand what challenges low vision people face and what strategies and tools they use when navigating such surface level changes. Using contextual inquiry, we interviewed and observed 14 low vision participants as they completed navigation tasks in two buildings and through two city blocks. The tasks involved walking in- and outdoors, across four staircases and two city blocks. We found that surface level changes were a source of uncertainty and even fear for all participants. Besides the white cane that many participants did not want to use, participants did not use technology in the study. Participants mostly used their vision, which was exhausting and sometimes deceptive. Our findings highlight the need for systems that support surface level changes and other depth-perception tasks; they should consider low vision people’s distinct experiences from blind people, their sensitivity to different lighting conditions, and leverage visual enhancements.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/sjitW-XYxrs?si=yVJZoCENbkyQVTIJ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2018enabling" class="col-sm-8"> <div class="title">Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation</div> <div class="title"> <em>CHI 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Cynthia L Bennett ,  Hrvoje Benko ,  Edward Cutrell ,  Christian Holz ,  Meredith Ringel Morris ,  and  Mike Sinclair </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/3173574.3173690" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/face-480.webp 480w,/assets/img/publication_preview/face-800.webp 800w,/assets/img/publication_preview/face-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/face.png" class="preview" width="150" height="130" alt="face.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2018face" class="col-sm-8"> <div class="title">A Face Recognition Application for People with Visual Impairments: Understanding Use beyond the Lab</div> <div class="title"> <em>CHI 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Shaomei Wu ,  Lindsay Reynolds ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3173574.3173789?casa_token=1ttDg814xkoAAAAA:dxQULHeu4tgDGZXAhDhgJ5Ed3elXwvbv4VWv7dEWaSFP_lmgYnemPylgF1c0aymZUvAXhcPiXWo_DQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user’s friends’ tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone’s camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone’s status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/description-480.webp 480w,/assets/img/publication_preview/description-800.webp 800w,/assets/img/publication_preview/description-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/description.png" class="preview" width="150" height="130" alt="description.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2017effect" class="col-sm-8"> <div class="title">The Effect of Computer-generated Descriptions on Photo-sharing Experiences of People with Visual Impairments</div> <div class="title"> <em>CSCW 2018</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Shaomei Wu ,  Lindsay Reynolds ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3134756?casa_token=irNy8hKlgDEAAAAA:GWkq8Q_7QDE6VNFqZKSZpueYqaE6FCCeix25LzsHbkH3QQu8euSbkyY46yQ4r_-brO4Sedlg7uDmnA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Like sighted people, visually impaired people want to share photographs on social networking services, but find it difficult to identify and select photos from their albums. We aimed to address this problem by incorporating state-of-the-art computer-generated descriptions into Facebook’s photo-sharing feature. We interviewed 12 visually impaired participants to understand their photo-sharing experiences and designed a photo description feature for the Facebook mobile application. We evaluated this feature with six participants in a seven-day diary study. We found that participants used the descriptions to recall and organize their photos, but they hesitated to upload photos without a sighted person’s input. In addition to basic information about photo content, participants wanted to know more details about salient objects and people, and whether the photos reflected their personal aesthetic. We discuss these findings from the lens of self-disclosure and self-presentation theories and propose new computer vision research directions that will better support visual content sharing by visually impaired people.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/models-480.webp 480w,/assets/img/publication_preview/models-800.webp 800w,/assets/img/publication_preview/models-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/models.png" class="preview" width="150" height="130" alt="models.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2017designing" class="col-sm-8"> <div class="title">Designing Interactions for 3D Printed Models with Blind People</div> <div class="title"> <em>ASSETS 2017</em>   </div> <div class="author"> Lei Shi ,  <em>Yuhang Zhao</em> ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3132525.3132549?casa_token=TFuTN_SmmJcAAAAA:fNzYa7-4nCuqsp2DC_HUHKUHJTThYQAhAGNwr1ylDiS0MgnFAPD10kmMTIH1T0Kj4iEUa4BroFfarQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users’ needs, and explore design opportunities. In the first section of the study, we observed participants’ behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants’ exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/markit-480.webp 480w,/assets/img/publication_preview/markit-800.webp 800w,/assets/img/publication_preview/markit-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/markit" class="preview" width="150" height="130" alt="markit" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shi2017markit" class="col-sm-8"> <div class="title">Markit and Talkit: a Low-barrier Toolkit to Augment 3D Printed Models with Audio Annotations</div> <div class="title"> <em>UIST 2017</em>   </div> <div class="author"> Lei Shi ,  <em>Yuhang Zhao</em> ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3126594.3126650?casa_token=BpTRFwCMXusAAAAA:qYYcOnB9R3Fr7wGv636oxEm0pkEE-11dY5_4PFbU8Ys4_W92UTHQqED7jXLBzMHteU1NOUK49OHKKQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit’s verbal cues. Talkit uses an RGB camera and a microphone to sense users’ inputs so it can run on a variety of devices. We evaluated Markit with eight sighted "makers" and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ARpotential-480.webp 480w,/assets/img/publication_preview/ARpotential-800.webp 800w,/assets/img/publication_preview/ARpotential-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/ARpotential.png" class="preview" width="150" height="130" alt="ARpotential.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2017understanding" class="col-sm-8"> <div class="title">Understanding Low Vision People’s Visual Perception on Commercial Augmented Reality Glasses</div> <div class="title"> <em>CHI 2017</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Michele Hu ,  Shafeka Hashash ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3025453.3025949?casa_token=aBgHPVW-B1IAAAAA:TutyBJh8RPtYAXffiLl7iHZpicj7o2metZtJ_OMs1xyLiT8DRaCy3P1vh9tWOmkekwmW9xpQxRmZjA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device’s constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people’s walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/device-480.webp 480w,/assets/img/publication_preview/device-800.webp 800w,/assets/img/publication_preview/device-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/device.png" class="preview" width="150" height="130" alt="device.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="szpiro2016people" class="col-sm-8"> <div class="title">How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities</div> <div class="title"> <em>ASSETS 2016</em>   <i class="fas fa-award" style="color: #b71c1c;"></i> <em>Best Paper Honorable Mention</em> </div> <div class="author"> Sarit Felicia Anais Szpiro ,  Shafeka Hashash ,  <em>Yuhang Zhao</em> ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/2982142.2982168?casa_token=yPY7rJWqn0AAAAAA:TPrZjAabnaNaj2sH3Uiv2dsme7sJmex8Y-QWO0sxmZCjbMmlqk0JMY77EZl3jVL4kjNz_dvNUmFHCw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/store-480.webp 480w,/assets/img/publication_preview/store-800.webp 800w,/assets/img/publication_preview/store-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/store.png" class="preview" width="150" height="130" alt="store.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="szpiro2016finding" class="col-sm-8"> <div class="title">Finding a Store, Searching for a Product: a Study of Daily Challenges of Low Vision People</div> <div class="title"> <em>UbiComp 2016</em>   </div> <div class="author"> Sarit Szpiro ,  <em>Yuhang Zhao</em> ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/2971648.2971723?casa_token=gqavoyDXWSMAAAAA:Vuk27wqkmnJcSfZUU5wgEukQBUpQXqieUEdCRkE0jUR698JImjF4ohniaujNkjlKc2SbhV1ngAo-Qg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual impairments encompass a range of visual abilities. People with low vision have functional vision and thus their experiences are likely to be different from people with no vision. We sought to answer two research questions: (1) what challenges do low vision people face when performing daily activities and (2) what aids (high- and low-tech) do low vision people use to alleviate these challenges? Our goal was to reveal gaps in current technologies that can be addressed by the UbiComp community. Using contextual inquiry, we observed 11 low vision people perform a wayfinding and shopping task in an unfamiliar environment. The task involved wayfinding and searching and purchasing a product. We found that, although there are low vision aids on the market, participants mostly used their smartphones, despite interface accessibility challenges. While smartphones helped them outdoors, participants were overwhelmed and frustrated when shopping in a store. We discuss the inadequacies of existing aids and highlight the need for systems that enhance visual information, rather than convert it to audio or tactile.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/0b8Q4832bjk?si=hzxGsfTwlAki_P4G" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2016cuesee" class="col-sm-8"> <div class="title">CueSee: Exploring Visual Cues for People with Low Vision to Facilitate a Visual Search Task</div> <div class="title"> <em>UbiComp 2016</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Sarit Szpiro ,  Jonathan Knighten ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2971648.2971730" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Visual search is a major challenge for low vision people. Conventional vision enhancements like magnification help low vision people see more details, but cannot indicate the location of a target in a visual search task. In this paper, we explore visual cues—a new approach to facilitate visual search tasks for low vision people. We focus on product search and present CueSee, an augmented reality application on a head-mounted display (HMD) that facilitates product search by recognizing the product automatically and using visual cues to direct the user’s attention to the product. We designed five visual cues that users can combine to suit their visual condition. We evaluated the visual cues with 12 low vision participants and found that participants preferred using our cues to conventional enhancements for product search. We also found that CueSee outperformed participants’ best-corrected vision in both time and accuracy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/foresee-480.webp 480w,/assets/img/publication_preview/foresee-800.webp 800w,/assets/img/publication_preview/foresee-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/foresee.png" class="preview" width="150" height="130" alt="foresee.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2015foresee" class="col-sm-8"> <div class="title">Foresee: A Customizable Head-mounted Vision Enhancement System for People with Low Vision</div> <div class="title"> <em>ASSETS 2015</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Sarit Szpiro ,  and  Shiri Azenkot </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2700648.2809865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Most low vision people have functional vision and would likely prefer to use their vision to access information. Recently, there have been advances in head-mounted displays, cameras, and image processing technology that create opportunities to improve the visual experience for low vision people. In this paper, we present ForeSee, a head-mounted vision enhancement system with five enhancement methods: Magnification, Contrast Enhancement, Edge Enhancement, Black/White Reversal, and Text Extraction; in two display modes: Full and Window. ForeSee enables users to customize their visual experience by selecting, adjusting, and combining different enhancement methods and display modes in real time. We evaluated ForeSee by conducting a study with 19 low vision participants who performed near- and far-distance viewing tasks. We found that participants had different preferences for enhancement methods and display modes when performing different tasks. The Magnification Enhancement Method and the Window Display Mode were popular choices, but most participants felt that combining several methods produced the best results. The ability to customize the system was key to enabling people with a variety of different vision abilities to improve their visual experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/-h1reziSo-Y?si=QNFjeFqKC16KSYZD" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2014qook" class="col-sm-8"> <div class="title">QOOK: Enhancing Information Revisitation for Active Reading with a Paper Book</div> <div class="title"> <em>TEI 2014</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Yongqiang Qin ,  Yang Liu ,  Siqi Liu ,  Taoshuai Zhang ,  and  Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2540930.2540977" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Revisiting information on previously accessed pages is a common activity during active reading. Both physical and digital books have their own benefits in supporting such activity according to their manipulation natures. In this paper, we introduce QOOK, a paper-book based interactive reading system, which integrates the advanced technology of digital books with the affordances of physical books to facilitate people’s information revisiting process. The design goals of QOOK are derived from the literature survey and our field study on physical and digital books respectively. QOOK allows page flipping just like on a real book and enables people to use electronic functions such as keyword searching, highlighting and bookmarking. A user study is conducted and the study results demonstrate that QOOK brings faster information revisiting and better reading experience to readers.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/ydlAKS-WlqU?si=2IK3Ow2lM9uzvRZh" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="huang2014focus" class="col-sm-8"> <div class="title">FOCUS: Enhancing Children’s Engagement in Reading by Using Contextual BCI Training Sessions</div> <div class="title"> <em>CHI 2014</em>   </div> <div class="author"> Jin Huang ,  Chun Yu ,  Yuntao Wang ,  <em>Yuhang Zhao</em> ,  Siqi Liu ,  Chou Mo ,  Jie Liu ,  Lie Zhang ,  and  Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2556288.2557339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Reading is an important aspect of a child’s development. Reading outcome is heavily dependent on the level of engagement while reading. In this paper, we present FOCUS, an EEG-augmented reading system which monitors a child’s engagement level in real time, and provides contextual BCI training sessions to improve a child’s reading engagement. A laboratory experiment was conducted to assess the validity of the system. Results showed that FOCUS could significantly improve engagement in terms of both EEG-based measurement and teachers’ subjective measure on the reading outcome.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 col-sm-4"> <iframe class="preview" width="150" height="130" src="https://www.youtube.com/embed/HupyGn2rJLo?si=x_QH0DWfaZn09rYI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div id="zhao2011picopet" class="col-sm-8"> <div class="title">PicoPet: " Real World" Digital Pet on a Handheld Projector</div> <div class="title"> <em>UIST 2011 Adjunct</em>   </div> <div class="author"> <em>Yuhang Zhao</em> ,  Chao Xue ,  Xiang Cao ,  and  Yuanchun Shi </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/10.1145/2046396.2046398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player’s daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yuhang Zhao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>