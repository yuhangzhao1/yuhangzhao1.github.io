---
---

@string{aps = {American Physical Society,}}

@inproceedings{lee2024CookAR,
  title={CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision},
  author={Lee, Jaewook and Tjahjadi, Andrew D and Kim, Jiho and Yu, Junpu and Park, Minji and Zhang, Jiawen and Froehlich, Jon E. and Tian, Yapeng and Zhao, Yuhang},
  booktitle={UIST 2024},
  book_abbr={UIST 2024},
  award={Belonging and Inclusion Best Paper Award},
  publisher={ACM},
  preview={cookar.png},
  abstract={Cooking is a central activity of daily living, supporting independence as well as mental and physical health. However, prior work has highlighted key barriers for people with low vision (LV) to cook, particularly around safely interacting with tools, such as sharp knives or hot pans. Drawing on recent advancements in computer vision (CV), we present <i>CookAR</i>, a head-mounted AR system with real-time object affordance augmentations to support safe and efficient interactions with kitchen tools. To design and implement CookAR, we collected and annotated the first egocentric dataset of kitchen tool affordances, fine-tuned an affordance segmentation model, and developed an AR system with a stereo camera to generate visual augmentations. To validate CookAR, we conducted a technical evaluation of our fine-tuned model as well as a qualitative lab study with 10 LV participants for suitable augmentation design. Our technical evaluation demonstrates that our model outperforms the baseline on our tool affordance dataset, while our user study indicates a preference for affordance augmentations over the traditional whole object augmentations.},
  pdf={https://dl.acm.org/doi/10.1145/3654777.3676449},
  code={https://github.com/makeabilitylab/CookAR?tab=readme-ov-file}
}

@inproceedings{zhao2024AIVision,
  title={AI-Vision: A Three-Layer Accessible Image Exploration System For People with Visual Impairments in China},
  author={Zhao, Kaixing and Lai, Rui and Guo, Bin and Liu, Le and He, Liang and Zhao, Yuhang},
  booktitle={IMWUT 2024},
  book_abbr={IMWUT 2024},
  publisher={ACM},
  preview={AIVision.png},
  abstract={Accessible image exploration systems are able to help people with visual impairments (PVI) to understand image content by providing different types of interactions. With the development of computer vision technologies, image exploration systems are supporting more fine-grained image content processing, including image segmentation, description and object recognition. However, in developing countries like China, it is still rare for PVI to widely rely on such accessible system. To better understand the usage situation of accessible image exploration system in China and improve the image understanding of PVI in China, we developed AI-Vision, an Android based hierarchical accessible image exploration system supporting the generations of image general description, local object description and metadata information. Our 7-day diary study with 10 PVI verified the usability of AI-Vision and also revealed a series of design implications for improving accessible image exploration systems similar to AI-Vision.},
  pdf={https://dl.acm.org/doi/abs/10.1145/3678537}
}

@inproceedings{hu2024really,
  title={"This Really Lets Us See the Entire World:" Designing a Conversational Telepresence Robot for Homebound Older Adults},
  author={Hu, Yaxin and Stegner, Laura and Kotturi, Yasmine and Zhang, Caroline and Peng, Yi-Hao and Huq, Faria and Zhao, Yuhang and Bigham, Jeffrey P and Mutlu, Bilge},
  booktitle={DIS 2024},
  book_abbr={DIS 2024},
  publisher={ACM},
  preview={hri.png},
  abstract={In this paper, we explore the design and use of conversational telepresence robots to help homebound older adults interact with the external world. An initial needfinding study (N=8) using video vignettes revealed older adults’ experiential needs for robot-mediated remote experiences such as exploration, reminiscence and social participation. We then designed a prototype system to support these goals and conducted a technology probe study (N=11) to garner a deeper understanding of user preferences for remote experiences. The study revealed user interactive patterns in each desired experience, highlighting the need of robot guidance, social engagements with the robot and the remote bystanders. Our work identifies a novel design space where conversational telepresence robots can be used to foster meaningful interactions in the remote physical environment. We offer design insights into the robot’s proactive role in providing guidance and using dialogue to create personalized, contextualized and meaningful experiences.},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3643834.3660710}
}

@inproceedings{wang2024gazeprompt,
  title={GazePrompt: Enhancing Low Vision People’s Reading Experience with Gaze-Aware Augmentations},
  author={Wang, Ru and Potter, Zach and Ho, Yun and Killough, Daniel and Zeng, Linda and Mondal, Sanbrita and Zhao, Yuhang},
  year={2024},
  booktitle={CHI 2024},
  book_abbr={CHI 2024},
  publisher= {ACM},
  video_preview={https://www.youtube.com/embed/SMbDfQtHQRE?si=-jXfU1VZ3Pifz38y},
  abstract={Reading is a challenging task for low vision people. 
While conventional low vision aids (e.g., magnification) offer certain support, they cannot fully address the difficulties faced by low vision users, such as locating the next line and distinguishing similar words. 
To fill this gap, we present <b>GazePrompt</b>, a gaze-aware reading aid that provides timely and targeted visual and audio augmentations based on users' gaze behaviors. GazePrompt includes two key features: (1) a Line-Switching support that highlights the line a reader intends to read; and (2) a Difficult-Word support that magnifies or reads aloud a word that the reader hesitates with.  
Through a study with 13 low vision participants who performed well-controlled reading-aloud tasks with and without GazePrompt, we found that GazePrompt significantly reduced participants' line switching time, reduced word recognition errors, and improved their subjective reading experiences. A follow-up silent-reading study showed that GazePrompt can enhance users' concentration and perceived comprehension of the reading contents. We further derive design considerations for future gaze-based low vision aids.},
  pdf={https://dl.acm.org/doi/10.1145/3613904.3642878}
}

@inproceedings{zhang2024exploring,
  title={Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field},
  author={Zhang, Kexin and Cochran, Brianna R and Chen, Ruijia and Hargung, Lance and Sprecher, Bryce and Tredinnick, Ross and Ponto, Kevin and Banerjee, Suman and Zhao, Yuhang},
  year={2024},
  booktitle={CHI 2024},
  book_abbr={CHI 2024},
  publisher= {ACM},
  preview={NIST.png},
  abstract={First responders navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. %First responders perform dangerous and time sensitive tasks in \change{mass-casualty incidents} hazardous, unfamiliar environments, making life-changing decisions in a split second. 
AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design spaces have not been thoroughly explored by involving various first responders. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed different first responders’ unique preferences and needs for AR cues and interactions, and identified desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment.},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3613904.3642195}
}

@inproceedings{Ning2024Spica,
  title={SPICA: Interactive Video Content Exploration through Augmented Audio Descriptions for Blind or Low-Vision Viewers},
  author={Ning, Zheng and Wimer, Brianna L and Jiang, Kaiwen and Ban, Jerrick and Tian, Yapeng and Zhao, Yuhang and Li, Toby Jia-Jun},
  year={2024},
  booktitle={CHI 2024},
  book_abbr={CHI 2024},
  publisher= {ACM},
  preview={spica.png},
  abstract={Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to access video content. However, conventional static ADs can leave out detailed information in videos, impose a high mental load, neglect the diverse needs and preferences of BLV users, and lack immersion. To tackle these challenges, we introduce SPICA, an AI-powered system that enables BLV users to interactively explore video content. Informed by prior empirical studies on BLV video consumption, SPICA offers novel interactive mechanisms for supporting temporal navigation of frame captions and spatial exploration of objects within key frames. Leveraging an audio-visual machine learning pipeline, SPICA augments existing ADs by adding interactivity, spatial sound effects, and individual object descriptions without requiring additional human annotation. Through a user study with 14 BLV participants, we evaluated the usability and usefulness of SPICA and explored user behaviors, preferences, and mental models when interacting with augmented ADs.},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3613904.3642632}
}

@inproceedings{Wang2024lowvision,
  title={How Do Low-Vision Individuals Experience Information Visualization?},
  author={Wang, Yanan and Zhao, Yuhang and Kim, Yea-Seul},
  year={2024},
  booktitle={CHI 2024},
  book_abbr={CHI 2024},
  publisher= {ACM},
  preview={lowvisionvis.png},
  abstract={In recent years, there has been a growing interest in enhancing the accessibility of visualizations for people with visual impairments. While much of the research has focused on improving accessibility for screen reader users, the specific needs of people with remaining vision (i.e., low-vision individuals) have been largely unaddressed. To bridge this gap, we conducted a qualitative study that provides insights into how low-vision individuals experience visualizations. We found that participants utilized various strategies to examine visualizations using the screen magnifiers and also observed that the default zoom level participants use for general purposes may not be optimal for reading visualizations. We identified that participants relied on their prior knowledge and memory to minimize the traversing cost when examining visualization. Based on the findings, we motivate a personalized tool to accommodate varying visual conditions of low-vision individuals and derive the design goals and features of the tool.},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3613904.3642188}
}

@inproceedings{Bei2024Starrescue,
  title={StarRescue: the Design and Evaluation of A Turn-Taking Collaborative Game for Facilitating Autistic Children's Social Skills},
  author={Bei, Rongqi and Liu, Yajie and Wang, Yihe and Huang, Yuxuan and Li, Ming and Zhao, Yuhang and Tong, Xin},
  year={2024},
  booktitle={CHI 2024},
  book_abbr={CHI 2024},
  publisher= {ACM},
  video_preview={https://www.youtube.com/embed/hbHJVuG1Hfs?si=gZaJvq1OlYh1Nsb7},
  abstract={Autism Spectrum Disorder (ASD) presents challenges in social interaction skill development, particularly in turn-taking. Digital interventions offer potential solutions for improving autistic children's social skills but often lack addressing specific collaboration techniques. Therefore, we designed a prototype of a turn-taking collaborative tablet game, StarRescue, which encourages children's distinct collaborative roles and interdependence while progressively enhancing sharing and mutual planning skills. We further conducted a controlled study with 32 autistic children to evaluate StarRescue's usability and potential effectiveness in improving their social skills. Findings indicated that StarRescue has great potential to foster turn-taking skills and social communication skills and also extended beyond the game. Additionally, we discussed implications for future work, such as including parents as game spectators and understanding autistic children's territory awareness in collaboration. Our study contributes a promising digital intervention for autistic children's turn-taking social skill development via a scaffolding approach and valuable design implications for future research.},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3613904.3642829}
}

@inproceedings{povinelli2024springboard,
  title={Springboard, Roadblock or "Crutch"?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality},
  author={Povinelli, Kassie and Zhao, Yuhang},
  booktitle={IEEE VR},
  book_abbr={IEEE VR 2024},
  abstract={Social virtual reality (VR) serves as a vital platform for transgender individuals to explore their identities through avatars and foster personal connections within online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. Prior research acknowledges this issue but overlooks the potential solution of voice changers. We interviewed 13 transgender and gender-nonconforming users of social VR platforms, focusing on their experiences with and without voice changers. We found that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through both hearing their modified voice and the reactions of others to their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified the technical barriers to current voice changer technology and potential improvements to alleviate the problems that transgender and gender-nonconforming users face.},
  pages={1--17},
  year={2024},
  preview={voicechanger.png},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494190}
}

@inproceedings{he2023multi,
  title={A Multi-modal Toolkit to Support DIY Assistive Technology Creation for Blind and Low Vision People},
  author={He, Liwen and Li, Yifan and Fan, Mingming and He, Liang and Zhao, Yuhang},
  booktitle={Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--3},
  year={2023},
  book_abbr={UIST 2023 Demo},
  video_preview={https://www.youtube.com/embed/1bdxNmVLZy8?si=2fsNY32gODTY5Syy},
  pdf={https://dl.acm.org/doi/abs/10.1145/3586182.3616646?casa_token=3e0TLtHQdpsAAAAA:-N0QUWy5m9qYYsq5fuwJEv58iwIU8ivPNOrw6A21anuVr4hMMcztkp2ahqHMbEcAt4r4qbG4y88iAA},
  abstract={We design and build A11yBits, a tangible toolkit that empowers blind and low vision (BLV) people to easily create personalized do-it-yourself assistive technologies (DIY-ATs). A11yBits includes (1) a series of Sensing modules to detect both environmental information and user commands, (2) a set of Feedback modules to send multi-modal feedback, and (3) two Base modules (Sensing Base and Feedback Base) to power and connect the sensing and feedback modules. The toolkit enables accessible and easy assembly via a “plug-and-play” mechanism. BLV users can select and assemble their preferred modules to create personalized DIY-ATs.}
}

@inproceedings{zhang2023diary,
  title={A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities},
  author={Zhang, Kexin and Deldari, Elmira and Yao, Yaxing and Zhao, Yuhang},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--17},
  year={2023},
  book_abbr={ASSETS 2023},
  preview={avatar.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3597638.3608388?casa_token=jLUHVfut7IgAAAAA:nenvMgIl7_F5YiBLHVdg_wicjNnATnoz7MdWa5_RDGI_ClXjFBCXuUTC0xh6zmDdUNYz2L8Sq4UuLA}
}

@inproceedings{wang2023practices,
  title={Practices and Barriers of Cooking Training for Blind and Low Vision People},
  author={Wang, Ru and Zhou, Nihan and Nguyen, Tam and Mondal, Sanbrita and Mutlu, Bilge and Zhao, Yuhang},
  booktitle={Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--5},
  year={2023},
  book_abbr={ASSETS 2023},
  preview={instructions.png},
  abstract={Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.},
  poster={https://dl.acm.org/doi/abs/10.1145/3597638.3614494},
  arxiv={2310.05396}
}

@inproceedings{wang2023understanding,
  title={Understanding How Low Vision People Read Using Eye Tracking},
  author={Wang, Ru and Zeng, Linxiu and Zhang, Xinyong and Mondal, Sanbrita and Zhao, Yuhang},
  booktitle={CHI 2023},
  book_abbr={CHI 2023},
  abstract={While being able to read with screen magnifiers, low vision people have slow and unpleasant reading experiences.Eye tracking has the potential to improve their experience by recognizing fine-grained gaze behaviors and providing more targeted enhancements. To inspire gaze-based low vision technology, <b>we investigate the suitable method to collect low vision users' gaze data via commercial eye trackers and thoroughly explore their challenges in reading based on their gaze behaviors</b>. With an improved calibration interface, we collected the gaze data of 20 low vision participants and 20 sighted controls who performed reading tasks on a computer screen; low vision participants were also asked to read with different screen magnifiers. We found that, with an accessible calibration interface and data collection method, commercial eye trackers can collect gaze data of comparable quality from low vision and sighted people. Our study identified low vision people’s unique gaze patterns during reading, building upon which, we propose design implications for gaze-based low vision technology.},
  pages={1--17},
  year={2023},
  preview={gazeoverlay.jpeg},
  pdf={https://dl.acm.org/doi/abs/10.1145/3544548.3581213?casa_token=65SwRttGZ3YAAAAA:Y8ZG_9uzrSQTQNfwjg_oKgokmo_stRcHJVRN7_i56WFLWSumf29oeRGBO8i2_lg9voe7BLkOxb4KpEc}
}

@inproceedings{zhao2023if,
  title={"If Sighted People Know, I Should Be Able to Know:" Privacy Perceptions of Bystanders with Visual Impairments around Camera-based Technology},
  author={Zhao, Yuhang and Yao, Yaxing and Fu, Jiaru and Zhou, Nihan},
  booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
  pages={4661--4678},
  book_abbr={USENIX Security 2023},
  year={2023},
  preview={bystander.png},
  abstract={Camera-based technology can be privacy-invasive, especially for bystanders who can be captured by the cameras but do not have direct control or access to the devices. The privacy threats become even more significant to bystanders with visual impairments (BVI) since they cannot visually discover the use of cameras nearby and effectively avoid being captured. While some prior research has studied visually impaired people's privacy concerns as direct users of camera-based assistive technologies, no research has explored their unique privacy perceptions and needs as bystanders. We conducted an in-depth interview study with 16 visually impaired participants to understand BVI's privacy concerns, expectations, and needs in different camera usage scenarios. A preliminary survey with 90 visually impaired respondents and 96 sighted controls was conducted to compare BVI and sighted bystanders' general attitudes towards cameras and elicit camera usage scenarios for the interview study. Our research revealed BVI's unique privacy challenges and perceptions around cameras, highlighting their needs for privacy awareness and protection. We summarized design considerations for future privacy-enhancing technologies to fulfill BVI's privacy needs.},
  pdf={https://www.usenix.org/conference/usenixsecurity23/presentation/zhao-yuhang}
}

@inproceedings{ji2022vrbubble,
  title={VRBubble: Enhancing Peripheral Awareness of Avatars for People with Visual Impairments in Social Virtual Reality},
  author={Ji, Tiger F and Cochran, Brianna and Zhao, Yuhang},
  booktitle={Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
  book_abbr={ASSETS 2022, CHI 2022 LBW},
  pages={1--17},
  year={2022},
  abstract={Social Virtual Reality (VR) is growing for remote socializing and collaboration. However, current social VR applications are not accessible to people with visual impairments (PVI) due to their focus on visual experiences. We aim to facilitate social VR accessibility by enhancing PVI's peripheral awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based VR technique that provides surrounding avatar information based on social distances. Based on Hall’s proxemic theory, VRBubble divides the social space with three Bubbles---Intimate, Conversation, and Social Bubble---generating spatial audio feedback to distinguish avatars in different bubbles and provide suitable avatar information. We provide three audio alternatives: earcons, verbal notifications, and real-world sound effects. PVI can select and combine their preferred feedback alternatives for different avatars, bubbles, and social contexts.},
  pdf={https://dl.acm.org/doi/abs/10.1145/3517428.3544821},
  video_preview={https://www.youtube.com/embed/1ITzOttCJkc?si=RiN1ntNEcplD9U5k},
  poster={https://dl.acm.org/doi/10.1145/3491101.3519657}
}

@inproceedings{zhang2022s,
  title={“It’s Just Part of Me:” Understanding Avatar Diversity and Self-presentation of People with Disabilities in Social Virtual Reality},
  author={Zhang, Kexin and Deldari, Elmira and Lu, Zhicong and Yao, Yaxing and Zhao, Yuhang},
  booktitle={Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--16},
  book_abbr={ASSETS 2022},
  abstract={In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD’s avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms.},
  pdf={https://dl.acm.org/doi/abs/10.1145/3517428.3544829},
  year={2022},
  preview={avatarrepresent.png}
}

@article{bhat2022confused,
  title={"I was Confused by It; It was Confused by Me:" Exploring the Experiences of People with Visual Impairments around Mobile Service Robots},
  author={Bhat, Prajna and Zhao, Yuhang},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW2},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY, USA},
  book_abbr={CSCW 2022},
  award={Diversity and Inclusion Recognition},
  preview={robot.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3555582},
  abstract={Mobile service robots have become increasingly ubiquitous. However, these robots can pose potential accessibility issues and safety concerns to people with visual impairments (PVI). We sought to explore the challenges faced by PVI around mainstream mobile service robots and identify their needs. Seventeen PVI were interviewed about their experiences with three emerging robots: vacuum robots, delivery robots, and drones. We comprehensively investigated PVI's robot experiences by considering their different roles around robots---direct users and bystanders. Our study highlighted participants' challenges and concerns about the accessibility, safety, and privacy issues around mobile service robots. We found that the lack of accessible feedback made it difficult for PVI to precisely control, locate, and track the status of the robots. Moreover, encountering mobile robots as bystanders confused and even scared the participants, presenting safety and privacy barriers. We further distilled design considerations for more accessible and safe robots for PVI.}
}

@inproceedings{stephenson2022sok,
  title={Sok: Authentication in Augmented and Virtual Reality},
  author={Stephenson, Sophie and Pal, Bijeeta and Fan, Stephen and Fernandes, Earlence and Zhao, Yuhang and Chatterjee, Rahul},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={267--284},
  year={2022},
  organization={IEEE},
  book_abbr={IEEE S&P 2022},
  pdf={https://ieeexplore.ieee.org/abstract/document/9833742?casa_token=lUPsA-7ItPoAAAAA:iHe0NSjfYDaRLXZZtYKlnHj5sDnc_2-I2OA20v9DussDup2Ia2tWiyk7lCiph-cdCCneZC--IQ},
  abstract={Augmented reality (AR) and virtual reality (VR) devices are emerging as prominent contenders to today’s personal computers. As personal devices, users will use AR and VR to store and access their sensitive data and thus will need secure and usable ways to authenticate. In this paper, we evaluate the state-of-the-art of authentication mechanisms for AR/VR devices by systematizing research efforts and practical deployments. By studying users’ experiences with authentication on AR and VR, we gain insight into the important properties needed for authentication on these devices. We then use these properties to perform a comprehensive evaluation of AR/VR authentication mechanisms both proposed in literature and used in practice. In all, we synthesize a coherent picture of the current state of authentication mechanisms for AR/VR devices. We draw on our findings to provide concrete research directions and advice on implementing and evaluating future authentication methods.},
  preview={sok.png}
}

@article{jung2021communicating,
  title={Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments},
  author={Jung, Crescentia and Mehta, Shubham and Kulkarni, Atharva and Zhao, Yuhang and Kim, Yea-Seul},
  journal={IEEE transactions on visualization and computer graphics},
  volume={28},
  number={1},
  pages={1095--1105},
  year={2021},
  publisher={IEEE},
  award={Best Paper Honorable Mention},
  book_abbr={IEEE VIS 2021},
  preview={vis.png},
  pdf={https://ieeexplore.ieee.org/abstract/document/9552938?casa_token=DEkn5d1il8cAAAAA:8HL2neCYlFi_yKCBT4yqC5Vz-r1k2Gao6l1OHECi5A4JETvSAEhfO_v7TxpekmD2LfSe3vOWwQ},
  abstract={Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users' cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.}
}

@inproceedings{weng2021facesight,
  title={Facesight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-facing Camera Vision},
  author={Weng, Yueting and Yu, Chun and Shi, Yingtian and Zhao, Yuhang and Yan, Yukang and Shi, Yuanchun},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2021},
  book_abbr={CHI 2021},
  preview={facesight.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3411764.3445484?casa_token=RaRZ-I6Jb74AAAAA:6nD-lPNAxiq94gSjHNsndk0magOHkQ7SIgQ3HEUUfMXQGeF-ZFTCgHtNQnB3y4hwNIcYySs-y_STzg},
  abstract={We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.}
}

@inproceedings{wen2020teacher,
  title={Teacher Views of Math E-learning Tools for Students with Specific Learning Disabilities},
  author={Wen, Zikai Alex and Silverstein, Erica and Zhao, Yuhang and Amog, Anjelika Lynne and Garnett, Katherine and Azenkot, Shiri},
  booktitle={Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--13},
  year={2020},
  book_abbr={ASSETS 2020},
  preview={edu.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3373625.3417029?casa_token=dDs3L4VS9tMAAAAA:Z7LLIcb8lSXmjGcZ__6QA6SJW3B_9L_GO68zkV5W5MKG9NO6HBcNGIjwaBm4ddwEZ7TvCaSWwW6I-Q},
  abstract={Many students with specific learning disabilities (SLDs) have difficulty learning math. To succeed in math, they need to receive personalized support from teachers. Recently, math e-learning tools that provide personalized math skills training have gained popularity. However, we know little about how well these tools help teachers personalize instruction for students with SLDs. To answer this question, we conducted semi-structured interviews with 12 teachers who taught students with SLDs in grades five to eight. We found that participants used math e-learning tools that were not designed specifically for students with SLDs. Participants had difficulty using these tools because of text-intensive user interfaces, insufficient feedback about student performance, inability to adjust difficulty levels, and problems with setup and maintenance. Participants also needed assistive technology for their students, but they had challenges in getting and using it. From our findings, we distilled design implications to help shape the design of more inclusive and effective e-learning tools.}
}

@inproceedings{zhao2020effectiveness,
  title={The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision},
  author={Zhao, Yuhang and Kupferstein, Elizabeth and Rojnirun, Hathaitorn and Findlater, Leah and Azenkot, Shiri},
  booktitle={Proceedings of the 2020 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2020},
  book_abbr={CHI 2020},
  preview={wayfinding.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3313831.3376516?casa_token=tjQeRWaX2ZsAAAAA:FjjX9Zm0XK-ypL57ajwALW3HKfgPuK3qypcU9W3s54Ekpx-6f8ww-INKa84MGpv5pNzTriJ_IHq6GA},
  abstract={Wayfinding is a critical but challenging task for people who have low vision, a visual impairment that falls short of blindness. Prior wayfinding systems for people with visual impairments focused on blind people, providing only audio and tactile feedback. Since people with low vision use their remaining vision, we sought to determine how audio feedback compares to visual feedback in a wayfinding task. We developed visual and audio wayfinding guidance on smartglasses based on de facto standard approaches for blind and sighted people and conducted a study with 16 low vision participants. We found that participants made fewer mistakes and experienced lower cognitive load with visual feedback. Moreover, participants with a full field of view completed the wayfinding tasks faster when using visual feedback. However, many participants preferred audio feedback because of its shorter learning curve. We propose design guidelines for wayfinding systems for low vision.}
}

@inproceedings{shi2020molder,
  title={Molder: an Accessible Design Tool for Tactile Maps},
  author={Shi, Lei and Zhao, Yuhang and Gonzalez Penuela, Ricardo and Kupferstein, Elizabeth and Azenkot, Shiri},
  booktitle={Proceedings of the 2020 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2020},
  book_abbr={CHI 2020},
  preview={molder.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3313831.3376431},
  abstract={Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.}
}

@inproceedings{zhao2019seeingvr,
  title={SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision},
  author={Zhao, Yuhang and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Ofek, Eyal and Wilson, Andrew D},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  book_abbr={CHI 2019},
  pages={1--14},
  year={2019},
  video_preview={https://www.youtube.com/embed/izmKY17CDhg?si=1mqWSMUC_qnAMU0x},
  abstract={Current virtual reality applications do not support people who have low vision, i.e., vision loss that falls short of complete blindness but is not correctable by glasses. We present SeeingVR, a set of 14 tools that enhance a VR application for people with low vision by providing visual and audio augmentations. A user can select, adjust, and combine different tools based on their preferences. Nine of our tools modify an existing VR application post hoc via a plugin without developer effort. The rest require simple inputs from developers using a Unity toolkit we created that allows integrating all 14 of our low vision support tools during development. Our evaluation with 11 participants with low vision showed that SeeingVR enabled users to better enjoy VR and complete tasks more quickly and accurately. Developers also found our Unity toolkit easy and convenient to use.},
  pdf={https://dl.acm.org/doi/abs/10.1145/3290605.3300341?casa_token=F95sK5_FZU0AAAAA:G04PVw98T40fHibgDJfk6bOv67nPxXsuQEc6-vxWNa3GjpHuX_-_oMyj7c7tfVfyeo0YCg8ClIyKDQ},
  code={https://github.com/microsoft/SeeingVRtoolkit}
}

@inproceedings{zhao2019designing,
  title={Designing AR Visualizations to Facilitate Stair Navigation for People with Low Vision},
  author={Zhao, Yuhang and Kupferstein, Elizabeth and Castro, Brenda Veronica and Feiner, Steven and Azenkot, Shiri},
  booktitle={Proceedings of the 32nd annual ACM symposium on user interface software and technology},
  pages={387--402},
  year={2019},
  book_abbr={UIST 2019},
  video_preview={https://www.youtube.com/embed/BbZNsa1ZeFQ?si=ghO2UiTbtblW00vN},
  pdf={https://dl.acm.org/doi/abs/10.1145/3332165.3347906},
  abstract={Navigating stairs is a dangerous mobility challenge for people with low vision, who have a visual impairment that falls short of blindness. Prior research contributed systems for stair navigation that provide audio or tactile feedback, but people with low vision have usable vision and don't typically use nonvisual aids. We conducted the first exploration of augmented reality (AR) visualizations to facilitate stair navigation for people with low vision. We designed visualizations for a projection-based AR platform and smartglasses, considering the different characteristics of these platforms. For projection-based AR, we designed visual highlights that are projected directly on the stairs. In contrast, for smartglasses that have a limited vertical field of view, we designed visualizations that indicate the user's position on the stairs, without directly augmenting the stairs themselves. We evaluated our visualizations on each platform with 12 people with low vision, finding that the visualizations for projection-based AR increased participants' walking speed. Our designs on both platforms largely increased participants' self-reported psychological security.}
}

@article{zhao2019designing,
  title={Designing and Evaluating a Customizable Head-mounted Vision Enhancement System for People with Low Vision},
  author={Zhao, Yuhang and Szpiro, Sarit and Shi, Lei and Azenkot, Shiri},
  journal={ACM Transactions on Accessible Computing (TACCESS)},
  volume={12},
  number={4},
  pages={1--46},
  year={2019},
  publisher={ACM New York, NY, USA},
  book_abbr={TACCESS 2019},
  preview={foreseeplus.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3361866},
  abstract={Recent advances in head-mounted displays (HMDs) present an opportunity to design vision enhancement systems for people with low vision, whose vision cannot be corrected with glasses or contact lenses. We aim to understand whether and how HMDs can aid low vision people in their daily lives. We designed ForeSee, an HMD prototype that enhances people’s view of the world with image processing techniques such as magnification and edge enhancement. We evaluated these vision enhancements with 20 low vision participants who performed four viewing tasks: image recognition and reading tasks from near- and far-distance. We found that participants needed to combine and adjust the enhancements to comfortably complete the viewing tasks. We then designed two input modes to enable fast and easy customization: speech commands and smartwatch-based gestures. While speech commands are commonly used for eyes-free input, our novel set of onscreen gestures on a smartwatch can be used in scenarios where speech is not appropriate or desired. We evaluated both input modes with 11 low vision participants and found that both modes effectively enabled low vision users to customize their visual experience on the HMD. We distill design insights for HMD applications for low vision and spur new research directions.}
}

@inproceedings{zhao2018looks,
  title={"It Looks Beautiful but Scary": How Low Vision People Navigate Stairs and Other Surface Level Changes},
  author={Zhao, Yuhang and Kupferstein, Elizabeth and Tal, Doron and Azenkot, Shiri},
  booktitle={Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={307--320},
  year={2018},
  award={Best Paper Honorable Mention},
  book_abbr={ASSETS 2018},
  preview={beautiful.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3234695.3236359},
  abstract={Walking in environments with stairs and curbs is potentially dangerous for people with low vision. We sought to understand what challenges low vision people face and what strategies and tools they use when navigating such surface level changes. Using contextual inquiry, we interviewed and observed 14 low vision participants as they completed navigation tasks in two buildings and through two city blocks. The tasks involved walking in- and outdoors, across four staircases and two city blocks. We found that surface level changes were a source of uncertainty and even fear for all participants. Besides the white cane that many participants did not want to use, participants did not use technology in the study. Participants mostly used their vision, which was exhausting and sometimes deceptive. Our findings highlight the need for systems that support surface level changes and other depth-perception tasks; they should consider low vision people's distinct experiences from blind people, their sensitivity to different lighting conditions, and leverage visual enhancements.}
}

@inproceedings{zhao2018enabling,
  title={Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation},
  author={Zhao, Yuhang and Bennett, Cynthia L and Benko, Hrvoje and Cutrell, Edward and Holz, Christian and Morris, Meredith Ringel and Sinclair, Mike},
  booktitle={Proceedings of the 2018 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2018},
  book_abbr={CHI 2018},
  video_preview={https://www.youtube.com/embed/sjitW-XYxrs?si=yVJZoCENbkyQVTIJ},
  pdf={https://dl.acm.org/doi/10.1145/3173574.3173690},
  abstract={Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.}
}

@inproceedings{zhao2018face,
  title={A Face Recognition Application for People with Visual Impairments: Understanding Use beyond the Lab},
  author={Zhao, Yuhang and Wu, Shaomei and Reynolds, Lindsay and Azenkot, Shiri},
  booktitle={Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2018},
  preview={face.png},
  book_abbr={CHI 2018},
  pdf={https://dl.acm.org/doi/abs/10.1145/3173574.3173789?casa_token=1ttDg814xkoAAAAA:dxQULHeu4tgDGZXAhDhgJ5Ed3elXwvbv4VWv7dEWaSFP_lmgYnemPylgF1c0aymZUvAXhcPiXWo_DQ},
  abstract={Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user's friends' tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone's camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone's status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications.}
}

@article{zhao2017effect,
  title={The Effect of Computer-generated Descriptions on Photo-sharing Experiences of People with Visual Impairments},
  author={Zhao, Yuhang and Wu, Shaomei and Reynolds, Lindsay and Azenkot, Shiri},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={1},
  number={CSCW},
  pages={1--22},
  year={2017},
  publisher={ACM New York, NY, USA},
  book_abbr={CSCW 2018},
  preview={description.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3134756?casa_token=irNy8hKlgDEAAAAA:GWkq8Q_7QDE6VNFqZKSZpueYqaE6FCCeix25LzsHbkH3QQu8euSbkyY46yQ4r_-brO4Sedlg7uDmnA},
  abstract={Like sighted people, visually impaired people want to share photographs on social networking services, but find it difficult to identify and select photos from their albums. We aimed to address this problem by incorporating state-of-the-art computer-generated descriptions into Facebook's photo-sharing feature. We interviewed 12 visually impaired participants to understand their photo-sharing experiences and designed a photo description feature for the Facebook mobile application. We evaluated this feature with six participants in a seven-day diary study. We found that participants used the descriptions to recall and organize their photos, but they hesitated to upload photos without a sighted person's input. In addition to basic information about photo content, participants wanted to know more details about salient objects and people, and whether the photos reflected their personal aesthetic. We discuss these findings from the lens of self-disclosure and self-presentation theories and propose new computer vision research directions that will better support visual content sharing by visually impaired people.}
}

@inproceedings{shi2017designing,
  title={Designing Interactions for 3D Printed Models with Blind People},
  author={Shi, Lei and Zhao, Yuhang and Azenkot, Shiri},
  booktitle={Proceedings of the 19th international acm sigaccess conference on computers and accessibility},
  pages={200--209},
  year={2017},
  book_abbr={ASSETS 2017},
  preview={models.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3132525.3132549?casa_token=TFuTN_SmmJcAAAAA:fNzYa7-4nCuqsp2DC_HUHKUHJTThYQAhAGNwr1ylDiS0MgnFAPD10kmMTIH1T0Kj4iEUa4BroFfarQ},
  abstract={Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.}
}

@inproceedings{shi2017markit,
  title={Markit and Talkit: a Low-barrier Toolkit to Augment 3D Printed Models with Audio Annotations},
  author={Shi, Lei and Zhao, Yuhang and Azenkot, Shiri},
  booktitle={Proceedings of the 30th annual acm symposium on user interface software and technology},
  pages={493--506},
  year={2017},
  book_abbr={UIST 2017},
  preview={markit},
  pdf={https://dl.acm.org/doi/abs/10.1145/3126594.3126650?casa_token=BpTRFwCMXusAAAAA:qYYcOnB9R3Fr7wGv636oxEm0pkEE-11dY5_4PFbU8Ys4_W92UTHQqED7jXLBzMHteU1NOUK49OHKKQ},
  abstract={As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit's verbal cues. Talkit uses an RGB camera and a microphone to sense users' inputs so it can run on a variety of devices. We evaluated Markit with eight sighted "makers" and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).}
}

@inproceedings{zhao2017understanding,
  title={Understanding Low Vision People's Visual Perception on Commercial Augmented Reality Glasses},
  author={Zhao, Yuhang and Hu, Michele and Hashash, Shafeka and Azenkot, Shiri},
  booktitle={Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages={4170--4181},
  year={2017},
  book_abbr={CHI 2017},
  preview={ARpotential.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3025453.3025949?casa_token=aBgHPVW-B1IAAAAA:TutyBJh8RPtYAXffiLl7iHZpicj7o2metZtJ_OMs1xyLiT8DRaCy3P1vh9tWOmkekwmW9xpQxRmZjA},
  abstract={People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device's constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people's walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.}
}

@inproceedings{szpiro2016people,
  title={How People with Low Vision Access Computing Devices: Understanding Challenges and Opportunities},
  author={Szpiro, Sarit Felicia Anais and Hashash, Shafeka and Zhao, Yuhang and Azenkot, Shiri},
  booktitle={Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={171--180},
  year={2016},
  book_abbr={ASSETS 2016},
  award={Best Paper Honorable Mention},
  preview={device.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/2982142.2982168?casa_token=yPY7rJWqn0AAAAAA:TPrZjAabnaNaj2sH3Uiv2dsme7sJmex8Y-QWO0sxmZCjbMmlqk0JMY77EZl3jVL4kjNz_dvNUmFHCw},
  abstract={Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.}
}

@inproceedings{szpiro2016finding,
  title={Finding a Store, Searching for a Product: a Study of Daily Challenges of Low Vision People},
  author={Szpiro, Sarit and Zhao, Yuhang and Azenkot, Shiri},
  booktitle={Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages={61--72},
  year={2016},
  book_abbr={UbiComp 2016},
  preview={store.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/2971648.2971723?casa_token=gqavoyDXWSMAAAAA:Vuk27wqkmnJcSfZUU5wgEukQBUpQXqieUEdCRkE0jUR698JImjF4ohniaujNkjlKc2SbhV1ngAo-Qg},
  abstract={Visual impairments encompass a range of visual abilities. People with low vision have functional vision and thus their experiences are likely to be different from people with no vision. We sought to answer two research questions: (1) what challenges do low vision people face when performing daily activities and (2) what aids (high- and low-tech) do low vision people use to alleviate these challenges? Our goal was to reveal gaps in current technologies that can be addressed by the UbiComp community. Using contextual inquiry, we observed 11 low vision people perform a wayfinding and shopping task in an unfamiliar environment. The task involved wayfinding and searching and purchasing a product. We found that, although there are low vision aids on the market, participants mostly used their smartphones, despite interface accessibility challenges. While smartphones helped them outdoors, participants were overwhelmed and frustrated when shopping in a store. We discuss the inadequacies of existing aids and highlight the need for systems that enhance visual information, rather than convert it to audio or tactile.}
}

@inproceedings{zhao2016cuesee,
  title={CueSee: Exploring Visual Cues for People with Low Vision to Facilitate a Visual Search Task},
  author={Zhao, Yuhang and Szpiro, Sarit and Knighten, Jonathan and Azenkot, Shiri},
  booktitle={Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages={73--84},
  year={2016},
  book_abbr={UbiComp 2016},
  video_preview={https://www.youtube.com/embed/0b8Q4832bjk?si=hzxGsfTwlAki_P4G},
  pdf={https://dl.acm.org/doi/10.1145/2971648.2971730},
  abstract={Visual search is a major challenge for low vision people. Conventional vision enhancements like magnification help low vision people see more details, but cannot indicate the location of a target in a visual search task. In this paper, we explore visual cues---a new approach to facilitate visual search tasks for low vision people. We focus on product search and present CueSee, an augmented reality application on a head-mounted display (HMD) that facilitates product search by recognizing the product automatically and using visual cues to direct the user's attention to the product. We designed five visual cues that users can combine to suit their visual condition. We evaluated the visual cues with 12 low vision participants and found that participants preferred using our cues to conventional enhancements for product search. We also found that CueSee outperformed participants' best-corrected vision in both time and accuracy.}
}

@inproceedings{zhao2015foresee,
  title={Foresee: A Customizable Head-mounted Vision Enhancement System for People with Low Vision},
  author={Zhao, Yuhang and Szpiro, Sarit and Azenkot, Shiri},
  booktitle={Proceedings of the 17th international ACM SIGACCESS conference on computers \& accessibility},
  pages={239--249},
  year={2015},
  book_abbr={ASSETS 2015},
  preview={foresee.png},
  pdf={https://dl.acm.org/doi/10.1145/2700648.2809865},
  abstract={Most low vision people have functional vision and would likely prefer to use their vision to access information. Recently, there have been advances in head-mounted displays, cameras, and image processing technology that create opportunities to improve the visual experience for low vision people. In this paper, we present ForeSee, a head-mounted vision enhancement system with five enhancement methods: Magnification, Contrast Enhancement, Edge Enhancement, Black/White Reversal, and Text Extraction; in two display modes: Full and Window. ForeSee enables users to customize their visual experience by selecting, adjusting, and combining different enhancement methods and display modes in real time. We evaluated ForeSee by conducting a study with 19 low vision participants who performed near- and far-distance viewing tasks. We found that participants had different preferences for enhancement methods and display modes when performing different tasks. The Magnification Enhancement Method and the Window Display Mode were popular choices, but most participants felt that combining several methods produced the best results. The ability to customize the system was key to enabling people with a variety of different vision abilities to improve their visual experience.}
}

@inproceedings{zhao2014qook,
  title={QOOK: Enhancing Information Revisitation for Active Reading with a Paper Book},
  author={Zhao, Yuhang and Qin, Yongqiang and Liu, Yang and Liu, Siqi and Zhang, Taoshuai and Shi, Yuanchun},
  booktitle={Proceedings of the 8th International Conference on Tangible, Embedded and Embodied Interaction},
  pages={125--132},
  year={2014},
  book_abbr={TEI 2014},
  video_preview={https://www.youtube.com/embed/-h1reziSo-Y?si=QNFjeFqKC16KSYZD},
  pdf={https://dl.acm.org/doi/10.1145/2540930.2540977},
  abstract={Revisiting information on previously accessed pages is a common activity during active reading. Both physical and digital books have their own benefits in supporting such activity according to their manipulation natures. In this paper, we introduce QOOK, a paper-book based interactive reading system, which integrates the advanced technology of digital books with the affordances of physical books to facilitate people's information revisiting process. The design goals of QOOK are derived from the literature survey and our field study on physical and digital books respectively. QOOK allows page flipping just like on a real book and enables people to use electronic functions such as keyword searching, highlighting and bookmarking. A user study is conducted and the study results demonstrate that QOOK brings faster information revisiting and better reading experience to readers.}
}

@inproceedings{huang2014focus,
  title={FOCUS: Enhancing Children's Engagement in Reading by Using Contextual BCI Training Sessions},
  author={Huang, Jin and Yu, Chun and Wang, Yuntao and Zhao, Yuhang and Liu, Siqi and Mo, Chou and Liu, Jie and Zhang, Lie and Shi, Yuanchun},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1905--1908},
  year={2014},
  book_abbr={CHI 2014},
  video_preview={https://www.youtube.com/embed/ydlAKS-WlqU?si=2IK3Ow2lM9uzvRZh},
  pdf={https://dl.acm.org/doi/10.1145/2556288.2557339},
  abstract={Reading is an important aspect of a child's development. Reading outcome is heavily dependent on the level of engagement while reading. In this paper, we present FOCUS, an EEG-augmented reading system which monitors a child's engagement level in real time, and provides contextual BCI training sessions to improve a child's reading engagement. A laboratory experiment was conducted to assess the validity of the system. Results showed that FOCUS could significantly improve engagement in terms of both EEG-based measurement and teachers' subjective measure on the reading outcome.}
}\
@inproceedings{zhao2011picopet,
  title={PicoPet: " Real World" Digital Pet on a Handheld Projector},
  author={Zhao, Yuhang and Xue, Chao and Cao, Xiang and Shi, Yuanchun},
  booktitle={Proceedings of the 24th annual ACM symposium adjunct on User interface software and technology},
  pages={1--2},
  year={2011},
  book_abbr={UIST 2011 Adjunct},
  video_preview={https://www.youtube.com/embed/HupyGn2rJLo?si=x_QH0DWfaZn09rYI},
  pdf={https://dl.acm.org/doi/10.1145/2046396.2046398},
  abstract={We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player's daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.}
}